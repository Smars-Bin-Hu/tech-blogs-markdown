> When Apache Hadoop first emerged, it quickly became the dominant force in the big data ecosystem. Many companies adopted it as their **primary big data solution**, and an entire ecosystem of tools and technologiesâ€”such as Hive and HBaseâ€”grew around it. However, in recent years, Hadoop has gradually been replaced by newer technologies. What led to this shift? Let's explore the reasons.
> 

# ğŸš€Â Introduction

<p align="center">
    <img src="./image/blog1-1.png" alt="image" width="40%">
</p>


Since the release of **Hadoop 2.x**, it has been widely used as a big data solution across industries. Hadoop consists of three **core components**:

- **HDFS (Hadoop Distributed File System)** â€“ Handles storage.
- **YARN (Yet Another Resource Negotiator)** â€“ Manages cluster resources.
- **MapReduce** â€“ Processes large-scale data using a batch computation model.

Hadoop's strength lies in its **distributed computing approach**, where storage and computation are managed within the same cluster. **DataNodes and NameNodes** work together to store and replicate massive datasets, while **ResourceManager and NodeManager** manage computing resources. **MapReduce follows a "Map & Reduce" paradigm**, enabling large-scale distributed computations.

Initially, Hadoop was the **go-to solution** for big data processing. Its open-source nature led to the development of **a rich ecosystem** of tools, including **Hive, HBase, and others**, forming a complete big data stack.

# ğŸš€Why Was Hadoop Replaced?

Hadoopâ€™s **three core components**â€”HDFS (storage), MapReduce (computation), and YARN (resource management)â€”each have their own limitations. Over time, these shortcomings led to the gradual decline of Hadoop's dominance.

## ğŸ”¥Â 1. Computation: The Fall of MapReduce

Originally, Hadoop used **MapReduce** as its computing engine. While **Java-based MapReduce** allowed developers to handle complex computation logic, **it required writing a lot of Java code**.

### The Rise of Hive

With the introduction of **Hive**, users could write SQL-like queries instead of Java code. **Hive's Driver and Compiler** translated SQL into MapReduce jobs, significantly improving **developer efficiency**. However, MapReduce had a fundamental performance limitation: 

- **It relied heavily on disk I/O**, leading to **slow processing speeds,**
- as well as even with optimizations like **Tez**, MapReduce still struggled to match modern computation engines.

### **Hive on Spark: Breaking MapReduceâ€™s Limits**

The introduction of **Apache Spark** marked a major breakthrough:

- **Spark is memory-based** and processes data up to **10 times faster than MapReduce**.
- It provides **rich APIs** (RDD, DataFrame) in Java, Scala, and Python, making it easier for developers to use.
- Hive adapted to Sparkâ€™s speed by allowing users to switch **MapReduce to Spark as the execution engine**.ğŸ‘‰ **This setup became known as "Hive on Spark".**

### **Shark: A Short-Lived Experiment**

Before **Spark SQL**, the **Shark project** allowed users to execute Hive SQL directly on Spark. Shark translated **Hive SQL into Spark RDD operations**, bypassing MapReduce entirely. However, as **Spark SQL** introduced its **Catalyst Optimizer and Tungsten Execution Engine**, Shark became obsolete and was eventually discontinued.

<p align="center">
    <img src="./image/blog1-2.png" alt="image" width="40%">
</p>

### The Shift to 100% Spark

Whether using **Hive on Spark** or **Shark**, users were still writing **HiveSQL (HQL)**. The only difference was that the execution engine changed to **Spark**, which offered better performance. However, the framework itself remained an **HQL-based architecture**, mainly designed for **data warehouses**. It supported the **Hive ecosystem**, including **Metastore, UDF, and HDFS storage**, which is why many companies continued using it for a long time.

Over time, companies began to **move away from HiveQL** and fully adopt **Spark**.

With **Spark computing directly**, users could execute data operations using **SparkCore (RDD API) and SparkSQL** without relying on **Hive to parse SQL and convert it into Spark tasks**. This **improved performance significantly** and also **enabled real-time processing (Streaming)**.

As a result, **Spark became the main computation engine for many companies**. By writing **SparkSQL**, they could analyze and process large-scale data more efficiently.

Many companies adopted a **new technology stack: HDFS + Spark + YARN**:

- **Data was still stored in HDFS within a Hadoop cluster.**
- **YARN managed computing resources using its ResourceManager.**
- **A dedicated Spark client submitted Spark jobs**, compiled Spark code, and sent the tasks to **YARN for execution**.

The only difference was that

**Spark replaced MapReduce as the execution engine.** ğŸ‘‰**This is what we call the "Spark on YARN" architecture.**

## ğŸ”¥ **2. Storage Layer**

## **HDFS Core Weaknesses**

| Weakness | Issue |
| --- | --- |
| **Small Files Problem** | HDFS is optimized for storing large files (e.g., GB-level logs) but struggles with many small files (e.g., KB-sized data). Since NameNode stores metadata for each file, too many small files put excessive pressure on it. |
| **Poor Random Read/Write Performance** | HDFS is designed for **large-scale sequential reads/writes** but is **not suitable for low-latency random access**, making it inefficient for database transactions or NoSQL workloads. |
| **High Storage Cost** | HDFS stores **three copies** of each data block by default, using **three times the storage space**, increasing storage costs. |
| **Single Point of Failure (NameNode Dependency)** | NameNode manages metadata. If NameNode fails, the entire HDFS may become unavailable (although HA mode reduces this risk). |
| **Limited Data Updates** | HDFS primarily supports **append-only writes**, meaning files **cannot be modified** (only rewritten entirely), making it unsuitable for **OLTP workloads**. |
| **Not Suitable for Low-Latency Queries** | HDFS requires **MapReduce or Spark** for data processing, leading to **high query latency**. It is not designed for interactive database-style queries. |
| **Lack of Scalability** | Since **storage and computing are coupled**, expanding HDFS requires **adding new Hadoop nodes**, rather than just increasing storage capacity. This increases **operational costs**. |

## **The Architecture of Compute-Storage Coupling**

In this architecture, the **Hadoop cluster** handles **HDFS storage**, while **Spark submits jobs to YARN** for computation. **Essentially, this means compute and storage remain tightly coupled within the same cluster.**

### **Spark on YARN: Job Execution Process**

<p align="center">
    <img src="./image/blog1-3.png" alt="image" width="40%">
</p>

1. **Spark Client Node submits a Spark job to YARN** (requesting resources from YARN).
2. **YARN ResourceManager schedules computing resources**:
    - It starts **ApplicationMaster**, which manages the job lifecycle.
    - It allocates **Executors** on **YARN NodeManagers**, which execute Spark tasks.
3. **Executors process the Spark tasks** and return results to the **Driver on the Spark Client Node**.
4. Since data is stored in **HDFS DataNodes**, Executors must **fetch data from these storage nodes** during computation.

**How Compute-Storage Coupling Works in Hadoop**

- **Each Worker Node** handles both **storage (HDFS DataNode)** and **computation (NodeManager + Executor)**.
- The **Master Node** (ResourceManager + NameNode) centrally manages **both compute resources and storage**.

**That being said, every compute unit and storage unit are coupling in one worker node. Hadoopâ€™s Data Locality optimization** ensures tasks run on nodes where data is stored, reducing network transfer. However, this architecture introduces several **limitations** in large-scale workloads.

## âŒ **Problems with Compute-Storage Coupling**

1. **Compute and storage cannot scale independently**:
    - **Storage scaling**: When data grows, new nodes must be added, even if more compute power isnâ€™t needed. This leads to **resource waste**.
    - **Compute scaling**: When computational demand increases but storage does not, **scaling the entire cluster is required**, increasing costs.
    - **High maintenance overhead**: Expanding Worker Nodes in Hadoop requires **manual configuration**, reducing **flexibility in scaling**.
2. **Compute and storage compete for resources**:
    - Compute tasks **must wait for storage node resources**, causing **processing delays**.
    - During peak loads (e.g., **AI training, large-scale data analysis**), **storage nodes can become bottlenecks**.
    - Since a **single server handles both HDFS storage and Spark execution**, it can lead to:
        - **I/O contention** (disk read/write competition).
        - **Insufficient memory** (data storage vs. Spark execution).
        - **CPU overload** (both computation and storage operations running simultaneously).
3. **Inefficient hardware utilization**:
    - **Unbalanced compute workload**: Some nodes are heavily loaded, while others remain idle.
    - **Uneven storage load**: HDFS follows a **three-replica strategy**, sometimes leading to **unequal data distribution**, increasing storage pressure on some nodes.

# ğŸš€ **Storage-Compute Separation**

The **Storage-Compute Separation** architecture **decouples storage from computation**, meaning computing tasks no longer depend on storage nodes. **Data storage is managed by an independent distributed storage cluster** (e.g., AWS S3), while **computation frameworks like Spark and Flink read data remotely to perform processing**.

<p align="center">
    <img src="./image/blog1-4.png" alt="image" width="40%">
</p>

## âœ… **Advantages of Storage-Compute Separation**

### **1. Independent Scaling of Storage and Compute**

**Compute nodes can scale up or down as needed**:

- During peak demand, more compute resources can be added quickly.
- During low demand, compute resources can be reduced to save costs.

**Storage nodes can also scale independently** without affecting compute resources.

This eliminates the **storage or compute bottleneck found in traditional Hadoop systems**.

### **2. Lower Operational Costs**

<p align="center">
    <img src="./image/blog1-5.png" alt="image" width="40%">
</p>

**Hadoop HDFS requires manual management** of NameNode, DataNode, storage expansion, and replication policies. If storage is insufficient, **new Hadoop Worker nodes must be added along with disks**. If NameNode fails, **HDFS may become unavailable unless HA (High Availability) is configured**.

**Cloud storage makes operations much simpler**:

- **For example, AWS S3 automatically handles storage management**â€”no need to maintain NameNode/DataNode manually.
- **Cloud storage can scale automatically**, eliminating the need to manually add machines.
- **Data remains available even after compute tasks finish**, whereas HDFS may require manual cleanup.

### 3. Replacing YARN with Kubernetes

<p align="center">
    <img src="./image/blog1-6.png" alt="image" width="40%">
</p>

In Hadoop ecosystems, compute frameworks like **Spark, Flink, and Hive** depend on **YARN** as the resource manager. This limits flexibility, making it difficult to integrate with **Kubernetes and Serverless environments**.

**With storage-compute separation, Kubernetes can manage compute resources**:

- **Run Spark on Kubernetes (Spark on K8s) instead of YARN**.
- **Run Flink on Kubernetes (Flink on K8s) for dynamic scaling**.
- **Kubernetes automatically handles scaling, failure recovery, and resource allocation**.

### **4. More Suitable for Cloud Computing & Multi-Region Deployment**

**Hadoop YARN is limited to a single data center**:

- Compute tasks **must run within the same data center** where YARN is deployed.
- Data migration to another data center requires **manual synchronization of HDFS data**.

**With storage-compute separation**:

- **Storage**: Object storage like **S3 / MinIO / Ceph** can be deployed across multiple regions and clouds.
- **Compute**: Compute clusters can be deployed **on Kubernetes**, allowing **on-demand scaling in any cloud**.

### **5. Cost Efficiency**

**Compute resources are released immediately after tasks finish**, reducing costs.

**Storage uses cost-effective cloud storage** instead of maintaining expensive HDFS clusters.

**Cloud-based pay-as-you-go pricing maximizes resource utilization**.

# ğŸš€ **Kubernetes in Cloud Data Engineering**

In modern cloud environments, **Kubernetes (K8s)** has become the standard for container orchestration.

It provides **high flexibility and scalability** for **data storage, processing, and streaming analytics**.

<p align="center">
    <img src="./image/blog1-7.png" alt="image" width="40%">
</p>

## **Distributed Data Storage**

Kubernetes allows managing and running **distributed storage systems**, such as:

- **Object storage (S3-compatible)**: MinIO, Ceph
- **Distributed file storage**: HDFS on Kubernetes
- **Relational databases**: PostgreSQL, MySQL
- **NoSQL databases**: Cassandra, MongoDB, Elasticsearch

### ğŸ“Â **Example: HDFS on Kubernetes**

Running **HDFS on a Kubernetes cluster** allows **dynamic scaling of DataNodes** to store **petabytes (PB) of data**. **Persistent Volume Claims (PVC)** help manage storage resources efficiently.

## Batch Data Processing

<p align="center">
    <img src="./image/blog1-8.png" alt="image" width="40%">
</p>

### ğŸ“Â **Example: Apache Spark on Kubernetes**

- **Spark Driver runs on the K8s Master node**, handling job scheduling.
- **Spark Executors run as Pods**, dynamically scaling based on demand.
- **Data is stored in HDFS or S3**, and Spark jobs read directly from storage.

This approach **removes the need for Hadoop YARN**. **Spark Executors run directly on Kubernetes Pods**, allowing **elastic scaling**. **When a job finishes, compute resources are released automatically**, reducing costs.

## Real-Time Stream Processing

<p align="center">
    <img src="./image/blog1-9.png" alt="image" width="40%">
</p>

### ğŸ“Â **Example: Flink on Kubernetes**

- **Kafka produces real-time data streams**.
- **Flink runs on Kubernetes**, with **TaskManagers deployed as Pods** to process streaming data.
- **Results are stored in NoSQL databases or data lakes (S3, HDFS)**.

This setup ensures **high availability**: If a **Flink Pod crashes, Kubernetes automatically restarts it**. Flink can **scale dynamically based on real-time data flow**, improving efficiency.

---

# å»HadoopåŒ–ï¼šå­˜ç®—åˆ†ç¦»å’ŒåŸºäºKubernetesçš„äº‘ç«¯æ•°æ®å·¥ç¨‹çš„å®è·µ

> Apache Hadoopä»å½“æ—¶çš„ä¸€å®¶ç‹¬å¤§ï¼Œè®©å¤§æ•°æ®çš„ç”Ÿæ€å›´ç»•åœ¨Hadoopä¸Šå±•å¼€ï¼Œæˆä¸ºå‡ ä¹æ‰€æœ‰å…¬å¸çš„å¤§æ•°æ®ä¸»æµè§£å†³æ–¹æ¡ˆã€‚åˆ°å¦‚ä»Šçš„é€æ¸è¢«æ›¿æ¢ï¼Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ï¼Ÿè°ˆè°ˆæˆ‘çš„ç†è§£ã€‚
> 

# ğŸš€Â Introduction

<p align="center">
    <img src="./image/blog1-1.png" alt="image" width="40%">
</p>

è‡ªä»Hadoop 2.xé—®ä¸–ï¼Œå®ƒå·²ç»ä½œä¸ºå¾ˆå¤šå…¬å¸çš„å¤§æ•°æ®è§£å†³æ–¹æ³•ã€‚Hadoopæœ‰ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

- HDFSè´Ÿè´£å­˜å‚¨
- Yarnè´Ÿè´£èµ„æºç®¡ç†
- MapReduceè´Ÿè´£è®¡ç®—

Hadoopçš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯åˆ©ç”¨äº†åˆ†å¸ƒå¼çš„æ€æƒ³ï¼Œè®©å­˜å‚¨èµ„æºå’Œè®¡ç®—èµ„æºåœ¨ä¸€ä¸ªé›†ç¾¤ä¸‹ç»Ÿä¸€ç®¡ç†ï¼Œé€šè¿‡DataNodeå’ŒNameNodeä¹‹é—´å·¥ä½œï¼Œæ¥å®ç°æµ·é‡æ•°æ®çš„å­˜å‚¨ã€å¤‡ä»½ï¼Œä»¥åŠé€šè¿‡ResourceManagerå’ŒNodeManagerï¼Œæ¥ç®¡ç†åˆ†å¸ƒå¼çš„æ•°æ®è®¡ç®—èµ„æºï¼ŒåŸºäºMapå’ŒReduceçš„åˆ†æ²»ï¼ˆDivide and Conquerï¼‰ç®—æ³•æ€æƒ³ï¼Œå¯¹æµ·é‡æ•°æ®è¿›è¡Œè®¡ç®—ã€‚

åœ¨æ—©æœŸï¼Œéšç€Hadoopæ¡†æ¶çš„å¼€æºï¼Œå‡ ä¹æ˜¯å¤§éƒ¨åˆ†å…¬å¸åœ¨å¤§æ•°æ®é¢†åŸŸçš„å”¯ä¸€åŒ–è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”è¡ç”Ÿå‡ºæ¥äº†å¾ˆå¤šå¾ˆå¤šæ–°çš„æŠ€æœ¯ï¼Œæ¯”å¦‚Hiveï¼ŒHBaseç­‰Hadoopç”Ÿæ€ä¸‹çš„æµè¡Œé¡¶çº§é¡¹ç›®ï¼Œå…±åŒç»„æˆäº†å¤§æ•°æ®çš„ä¸€å¥—æˆç†Ÿçš„è§£å†³æ–¹æ¡ˆã€‚

# ğŸš€Â Hadoopçš„å¼Šç«¯

Hadoopçš„ä¸‰å¤§ç»„ä»¶ï¼šHDFSè´Ÿè´£å­˜å‚¨ã€MapReduceè´Ÿè´£è®¡ç®—ã€Yarnè´Ÿè´£èµ„æºç®¡ç†ï¼Œä»–ä»¬æœ‰ä»€ä¹ˆç¼ºç‚¹ï¼Œä¸ºä»€ä¹ˆæœ€åè¢«é€æ¸å–ä»£äº†ï¼Ÿ

## ğŸ”¥Â 1. è®¡ç®—å±‚ï¼šMapReduceçš„é™¨è½

åŸå…ˆHadoopæ˜¯åŸºäºMapReduceè¿›è¡Œè®¡ç®—ï¼ŒMapReduceå¯¹Javaçš„æ”¯æŒï¼Œä½¿å¾—å¯ä»¥è§£å†³å¾ˆå¤šå¾ˆå¤æ‚çš„è®¡ç®—é€»è¾‘ï¼Œä½†æ˜¯ç”¨æˆ·è¿˜æ˜¯éœ€è¦æ‰‹å†™å¤§é‡çš„Javaä»£ç ã€‚

### Hiveçš„å…´èµ·

éšç€Hiveçš„å‡ºç°ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡SQLæ¥å®ç°å¤æ‚é€»è¾‘çš„è®¡ç®—ï¼ŒHive Driverå’ŒHive Compilerç­‰ç»„ä»¶è´Ÿè´£è§£æç”¨æˆ·æäº¤çš„SQLè¯­å¥å¹¶ä¸”è½¬æ¢ä¸ºMapReduceä»»åŠ¡ï¼Œå¤§å¤§æå‡äº†å¼€å‘è€…çš„å¼€å‘æ•ˆç‡ã€‚ä½†æ˜¯MapReduceå› ä¸ºæ˜¯åŸºäºç£ç›˜è®¡ç®—çš„ï¼Œå…¶æ€§èƒ½éå¸¸æœ‰é™ã€‚å³ä½¿éšç€Tezè®¡ç®—å¼•æ“çš„å‡ºç°ï¼Œè®¡ç®—æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œä»ç„¶æ•´ä½“å·®å¼ºäººæ„ã€‚

### Hive on Spark

Sparkçš„å‡ºç°ï¼Œå¯¹MapReduceè¿›è¡Œäº†çªç ´ã€‚Sparkæ˜¯åŸºäºå†…å­˜è®¡ç®—ï¼Œå¹¶ä¸”æä¾›äº†ä¸°å¯Œçš„RDDç®—å­è®¡ç®—ï¼Œè®©Java/Scale/Pythonå¼€å‘è€…ï¼Œå¯ä»¥å†™RDDå‡½æ•°æˆ–è€…DSLå‡½æ•°ï¼Œæ¥å®Œæˆå¯¹Sparkçš„å¼€å‘ï¼Œå¹¶ä¸”é€šè¿‡å†…å­˜è®¡ç®—ï¼Œè¿‘100å€äºä¹‹å‰MapReduceçš„æ‰§è¡Œæ•ˆç‡ã€‚

Hiveå½“æ—¶ä½œä¸ºå¤§æ•°æ®çš„ä¸€ä¸ªä¸»æµæ•°ä»“ç®¡ç†å·¥å…·ï¼ŒåŸæœ¬æ˜¯é€šè¿‡Metastoreå­˜å‚¨å…ƒæ•°æ®ï¼Œæ¥è¯»å–HDFSä¸Šçš„æ•°æ®ï¼Œé€šè¿‡SQLç¼–è¯‘å™¨æ¥æ‰§è¡ŒMapReduceä»»åŠ¡ï¼Œåæ¥ï¼Œ**Hiveé€šè¿‡å¯¹è®¡ç®—å¼•æ“çš„é…ç½®ï¼Œè®©è®¡ç®—é€šè¿‡Sparkæ¥è¿›è¡Œï¼Œè€Œä¸æ˜¯MapReduceæˆ–è€…Tezã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„Hive on Sparkã€‚**

### Sharkï¼ˆå·²è¢«æ·˜æ±°ï¼‰

è¿™ä¸ªæ¦‚å¿µï¼Œå¯èƒ½å¤§éƒ¨åˆ†èµ„æ·±çš„Data Engineeréƒ½å¬è¯´è¿‡ï¼Œæ˜¯Sparkç¤¾åŒºæ—©æœŸçš„ä¸€ä¸ªé¡¹ç›®ï¼Œç›®çš„æ˜¯è®©ç”¨æˆ·æäº¤Hive SQLåœ¨Sparkä¸Šè¿è¡Œï¼ˆç”¨æˆ·å†™ Hive SQLï¼ŒShark è§£æå¹¶è½¬æ¢ä¸º Spark RDD ä»»åŠ¡ï¼‰ä½†æ˜¯åæ¥å‡ºç°äº†SparkSQLï¼Œè¿™ä¸ªé¡¹ç›®å°±åœæ­¢äº†ã€‚

<p align="center">
    <img src="./image/blog1-2.png" alt="image" width="40%">
</p>

### The Shift to 100% Spark

Hive on Sparkæˆ–è€…Sharkä¹Ÿå¥½ï¼Œç”¨æˆ·ä»ç„¶å†™çš„æ˜¯HiveSQLï¼ˆHQLï¼‰ï¼Œåªæ˜¯ä»»åŠ¡æäº¤åˆ°Sparkæ‰§è¡Œå¼•æ“è€Œå·²ï¼Œç”±æ€§èƒ½æ›´å¼ºçš„Sparkè¿›è¡Œè®¡ç®—ã€‚ä½†æ˜¯æ•´ä¸ªæ¡†æ¶ä»ç„¶è¿˜æ˜¯HQLæ¡†æ¶ï¼Œè¿™å¥—æ¡†æ¶ä¸»è¦æ˜¯é€‚ç”¨äºæ•°æ®ä»“åº“åœºæ™¯ï¼Œæ”¯æŒhiveç”Ÿæ€ï¼Œå¦‚Metastoreã€UDFã€HDFSå­˜å‚¨ï¼Œæ‰€ä»¥èƒ½ä¸€ç›´åœ¨å¾ˆå¤šå…¬å¸æ²¿ç”¨åˆ°ç°åœ¨ã€‚ä½†æ˜¯æ¸æ¸åœ°ï¼Œäººä»¬å¼€å§‹æ·˜æ±°HiveQLçš„æ¡†æ¶ï¼Œè½¬è€Œ100%æŠ•å…¥åˆ°Sparkçš„æ€€æŠ±ã€‚

è€ŒSparkç›´æ¥è®¡ç®—ï¼Œæ˜¯ç”¨æˆ·ç›´æ¥é€šè¿‡SparkCoreï¼ˆRDD APIï¼‰ å’Œ SparkSQLç›´æ¥æ¥æ‰§è¡Œæ•°æ®çš„è®¡ç®—ã€‚è¿™æ ·æ€§èƒ½ä¼šæ›´åŠ æé«˜ï¼Œå› ä¸ºçœå»äº†Hiveè§£æSQLå’Œç”ŸæˆSparkä»»åŠ¡çš„ç¯èŠ‚ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå®æ—¶è®¡ç®—ï¼ˆStreamingï¼‰ã€‚

è‡³æ­¤ï¼ŒSparké€æ¸æˆä¸ºäº†å¾ˆå¤šå…¬å¸çš„ä¸»æµè®¡ç®—å¼•æ“ï¼Œå¤§å®¶é€šè¿‡ç›´æ¥å†™SparkSQLï¼Œå°±å®ç°äº†å¯¹æµ·é‡æ•°æ®çš„åˆ†æè®¡ç®—ã€‚å› æ­¤åé¢å¾ˆå¤šå…¬å¸çš„ä¸€ä¸ªæŠ€æœ¯æ¶æ„æ˜¯HDFS+Spark+Yarnï¼šæ•´ä½“æ•°æ®ä»ç„¶æ˜¯å­˜åœ¨Hadoopé›†ç¾¤ä¸Šçš„HDFSï¼Œèµ„æºç®¡ç†ä¹Ÿæ˜¯åœ¨Hadoopé›†ç¾¤ä¸ŠåŸºäºYarnçš„ResourceManageræ¥å®ç°ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªä¸“é—¨çš„Sparkå®¢æˆ·ç«¯ï¼Œè´Ÿè´£æäº¤Spark Jobï¼Œè§£æSparkä»£ç å¹¶ä¸”æäº¤ç»™Hadoopé›†ç¾¤çš„Yarnæ¥è¿›è¡Œè®¡ç®—ï¼Œåªæ˜¯è®¡ç®—å¼•æ“æ˜¯Sparkè€Œå·²ã€‚**è¿™å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„Spark on Yarnçš„æ¶æ„ã€‚**

## ğŸ”¥Â äºŒã€å­˜å‚¨å±‚é¢

## HDFSæ ¸å¿ƒåŠ£åŠ¿

| åŠ£åŠ¿ | å…·ä½“é—®é¢˜ |
| --- | --- |
| **å°æ–‡ä»¶é—®é¢˜ï¼ˆSmall Files Issueï¼‰** | HDFS é€‚ç”¨äºå­˜å‚¨å¤§æ–‡ä»¶ï¼ˆå¦‚ GB çº§æ—¥å¿—ï¼‰ï¼Œä½†ä¸é€‚åˆå­˜å‚¨å¤§é‡å°æ–‡ä»¶ï¼ˆå¦‚ KB çº§æ•°æ®ï¼‰ï¼Œå› ä¸º NameNode éœ€è¦å­˜å‚¨æ–‡ä»¶å…ƒæ•°æ®ï¼Œå¯¼è‡´å…ƒæ•°æ®å‹åŠ›è¿‡å¤§ã€‚ |
| ä½éšæœºè¯»å†™æ€§èƒ½ | HDFS é€‚ç”¨äº**å¤§è§„æ¨¡é¡ºåºè¯»å†™**ï¼Œä¸é€‚åˆ**ä½å»¶è¿Ÿéšæœºè®¿é—®**ï¼Œå› æ­¤æ— æ³•é«˜æ•ˆæ”¯æŒæ•°æ®åº“äº‹åŠ¡æˆ– NoSQL ä¸šåŠ¡ã€‚ |
| ä¸‰å‰¯æœ¬å­˜å‚¨æˆæœ¬é«˜ | é»˜è®¤æ¯ä¸ªæ•°æ®å—å­˜ 3 ä»½ï¼Œå ç”¨ 3 å€å­˜å‚¨ç©ºé—´ï¼Œå­˜å‚¨æˆæœ¬è¾ƒé«˜ã€‚ |
| å•ç‚¹æ•…éšœï¼ˆNameNode ä¾èµ–ï¼‰ | NameNode å­˜å‚¨å…ƒæ•°æ®ï¼Œå¦‚æœ NameNode æ•…éšœï¼Œæ•´ä¸ª HDFS å¯èƒ½ä¸å¯ç”¨ï¼ˆå°½ç®¡ HA æ¨¡å¼å¯ç¼“è§£ï¼‰ã€‚ |
| æ•°æ®æ›´æ–°ä¸çµæ´» | HDFS ä¸»è¦æ”¯æŒ **è¿½åŠ å†™ï¼ˆAppend Onlyï¼‰**ï¼Œä¸æ”¯æŒæ–‡ä»¶ä¿®æ”¹ï¼ˆåªèƒ½é‡å†™æ•´ä¸ªæ–‡ä»¶ï¼‰ï¼Œä¸é€‚åˆ OLTP ä¸šåŠ¡ã€‚ |
| ä¸é€‚ç”¨äºä½å»¶è¿ŸæŸ¥è¯¢ | HDFS éœ€è¦ MapReduce/Spark å¤„ç†æ•°æ®ï¼Œè®¡ç®—å»¶è¿Ÿè¾ƒé«˜ï¼Œä¸é€‚ç”¨äºäº¤äº’å¼æŸ¥è¯¢ï¼ˆå¦‚æ•°æ®åº“çº§æŸ¥è¯¢ï¼‰ã€‚ |
| æ‰©å±•ä¸å¤Ÿçµæ´» | è®¡ç®—å’Œå­˜å‚¨è€¦åˆï¼Œæ‰©å±• HDFS éœ€è¦å¢åŠ æ•´ä¸ª Hadoop æœåŠ¡å™¨ï¼Œè€Œä¸æ˜¯å•ç‹¬æ‰©å±•å­˜å‚¨ã€‚è¿™ç§è¿ç»´çš„æˆæœ¬èŠ±è´¹æ¯”è¾ƒå¤šã€‚ |

## å­˜ç®—è€¦åˆçš„æ¶æ„

ä½ å¯èƒ½å‘ç°äº†è¿™å¥—æ¶æ„çš„æ¨¡å¼ï¼šHadoopé›†ç¾¤è´Ÿè´£HDFSçš„å­˜å‚¨ï¼Œå¹¶ä¸”Sparkæäº¤jobåˆ°Hadoopé›†ç¾¤Yarnä¸Šï¼Œè¿›è¡Œè®¡ç®—ï¼Œå®è´¨ä¸Šè¿™ä»ç„¶æ˜¯ä¸€å¥—å­˜å‚¨å’Œè®¡ç®—è€¦åˆçš„æ¡†æ¶ã€‚

### **Spark on Yarnçš„Job Executionæµç¨‹**

<p align="center">
    <img src="./image/blog1-3.png" alt="image" width="40%">
</p>

1. Spark Client Nodeåœ¨æäº¤Sparkä»»åŠ¡ç»™Yarnåï¼ˆå‘Yarnè¯·æ±‚èµ„æºï¼‰
2. Yarn ResourceManagerå°†è´Ÿè´£è°ƒåº¦è®¡ç®—èµ„æºï¼š 
    - å¯åŠ¨ApplicationMasterå¹¶ä¸”ç”³è¯·èµ„æºï¼Œ
    - å¹¶åœ¨Yarn NodeManagerä¸Šå¯åŠ¨Executorï¼ˆè´Ÿè´£æ‰§è¡ŒSparkä»»åŠ¡çš„è¿›ç¨‹ï¼‰
3. Executoræ‰§è¡ŒSparkè®¡ç®—ä»»åŠ¡ï¼Œç„¶åæŠŠç»“æœè¿”å›ç»™Driver on the Spark Client nodeã€‚
4. åœ¨è¿™ä¸ªæ¶æ„ä¸­ï¼Œæ•°æ®ä»ç„¶å­˜æ”¾åœ¨HDFS Data Nodeï¼Œæ‰€ä»¥Executorä»ç„¶è¦é€šè¿‡è®¿é—®DataNodeæ¥è¯»å–æ•°æ®ã€‚

æ‰€ä»¥åœ¨Hadoopé›†ç¾¤ä¸Šï¼Œæ¯ä¸ªWorker Nodeä»ç„¶æ˜¯æ—¢è´Ÿè´£å­˜å‚¨ï¼ˆHDFS Data Nodeï¼‰ï¼Œåˆè´Ÿè´£è®¡ç®—ï¼ˆNodeManager + Executorï¼‰ï¼ŒåŒæ—¶ç”±MasterNodeï¼ˆResourceManager + DataNodeï¼‰è´Ÿè´£ç»Ÿä¸€çš„èµ„æºå’Œå­˜å‚¨ç®¡ç†ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼ŒHadoopçš„æ—©æœŸæ¶æ„ä¸­ï¼Œè®¡ç®—å’Œå­˜å‚¨æ˜¯ç»‘å®šåœ¨åŒä¸€ç»„æœåŠ¡å™¨ä¸Šçš„ï¼Œæ¯ä¸ªDataNodeæ—¢å­˜å‚¨æ•°æ®åˆæ‰§è¡Œè®¡ç®—ä»»åŠ¡ã€‚å°½ç®¡Hadoopé€šè¿‡Data Localityæ¥ä¼˜åŒ–è®¡ç®—ï¼ˆè®©ä»»åŠ¡å°½é‡åœ¨å­˜å‚¨æ•°æ®çš„èŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œä»¥å‡å°‘ç½‘ç»œä¼ è¾“ï¼‰ï¼Œä½†è¿™ä¹Ÿä¸å¯é¿å…åœ°ï¼Œåœ¨ä¸€äº›åœºæ™¯ä¸­å‡ºç°äº†å¾ˆå¤šé—®é¢˜ã€‚

## âŒÂ å­˜ç®—è€¦åˆå¸¦æ¥çš„å¼Šç«¯

1. **å­˜å‚¨å’Œè®¡ç®—èµ„æºæ— æ³•ç‹¬ç«‹æ‰©å±•**ï¼š
    - **å­˜å‚¨æ‰©å±•**ï¼šå¦‚æœæ•°æ®å¢é•¿ï¼Œéœ€è¦å¢åŠ æ–°çš„èŠ‚ç‚¹ï¼Œä½†è®¡ç®—èµ„æºå¯èƒ½è¿‡å‰©ï¼Œå¯¼è‡´èµ„æºæµªè´¹ã€‚
    - **è®¡ç®—æ‰©å±•**ï¼šå¦‚æœè®¡ç®—ä»»åŠ¡å¢åŠ ï¼Œä½†å­˜å‚¨ä¸éœ€è¦å¢åŠ ï¼Œä»ç„¶éœ€è¦æ‰©å®¹æ•´ä¸ªé›†ç¾¤ï¼Œæˆæœ¬å¢åŠ ã€‚
    - å¹¶ä¸”Hadoopçš„Nodeç»´æŠ¤æˆæœ¬æ¯”è¾ƒå¤§ï¼Œworknodeçš„æ‰©å®¹ï¼Œéƒ½éœ€è¦æ‰‹åŠ¨è¿ç»´ï¼Œè®©æ•´ä¸ªæ‰©å®¹å’Œç¼©å®¹çš„çµæ´»æ€§ä¸‹é™å¾ˆå¤šã€‚
2. **å­˜å‚¨å’Œè®¡ç®—èµ„æºå†²çª**ï¼š
    - è®¡ç®—ä»»åŠ¡éœ€è¦æ’é˜Ÿç­‰å¾…å­˜å‚¨èŠ‚ç‚¹çš„èµ„æºï¼Œå½±å“è®¡ç®—æ•ˆç‡ã€‚
    - å½“ä»»åŠ¡é‡é«˜å³°æ—¶ï¼ˆå¦‚ AI è®­ç»ƒã€å¤§è§„æ¨¡æ•°æ®åˆ†æï¼‰ï¼Œå­˜å‚¨èŠ‚ç‚¹å¯èƒ½æˆä¸ºè®¡ç®—ç“¶é¢ˆã€‚
    - åŒä¸€å°æœåŠ¡å™¨æ—¢è´Ÿè´£å­˜å‚¨ HDFS æ•°æ®ï¼Œåˆè´Ÿè´£è®¡ç®— Spark ä»»åŠ¡ï¼Œå¯èƒ½å¯¼è‡´ **I/O äº‰ç”¨ã€å†…å­˜ä¸è¶³ã€CPU è¿‡è½½** ç­‰é—®é¢˜ã€‚
3. **ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ä½**ï¼š
    - **è®¡ç®—è´Ÿè½½ä¸å‡è¡¡**ï¼šéƒ¨åˆ†èŠ‚ç‚¹è´Ÿè½½é«˜ï¼Œè€Œå…¶ä»–èŠ‚ç‚¹ç©ºé—²ã€‚
    - **å­˜å‚¨è´Ÿè½½ä¸å‡è¡¡**ï¼šHDFS é‡‡ç”¨ **ä¸‰å‰¯æœ¬æœºåˆ¶**ï¼Œå¯¼è‡´æ•°æ®åˆ†å¸ƒå¯èƒ½ä¸å‡è¡¡ï¼ŒæŸäº›èŠ‚ç‚¹å­˜å‚¨å‹åŠ›å¤§ã€‚

# ğŸš€Â å­˜ç®—åˆ†ç¦»

å­˜ç®—åˆ†ç¦»ï¼ˆStorage-Compute Separationï¼‰æ¶æ„å°†å­˜å‚¨å’Œè®¡ç®—æ‹†åˆ†ï¼Œè®¡ç®—ä»»åŠ¡ä¸ä¾èµ–äºå­˜å‚¨èŠ‚ç‚¹ã€‚æ•°æ®å­˜å‚¨èŠ‚ç‚¹åœ¨ç‹¬ç«‹çš„åˆ†å¸ƒå¼å­˜å‚¨é›†ç¾¤ä¸Šï¼ˆå¦‚ AWS S3ï¼‰ï¼Œè®¡ç®—æ¡†æ¶èŠ‚ç‚¹ï¼ˆSparkã€Flinkï¼‰ä»è¿œç¨‹å­˜å‚¨è¯»å–æ•°æ®æ‰§è¡Œè®¡ç®—ã€‚

<p align="center">
    <img src="./image/blog1-4.png" alt="image" width="40%">
</p>

## âœ…å­˜å‚¨è®¡ç®—åˆ†æçš„ä¼˜åŠ¿

1. å­˜å‚¨å’Œè®¡ç®—å¯ä»¥ç‹¬ç«‹æ‹“å±•å’Œæ”¶ç¼©
    - è®¡ç®—èŠ‚ç‚¹å¯ä»¥æŒ‰éœ€æ‹“å±•ï¼Œæ¯”å¦‚è®¡ç®—éœ€æ±‚é«˜å³°æ—¶æœŸï¼Œå¯ä»¥å¿«é€Ÿå¢åŠ è®¡ç®—èµ„æºæ¥æ»¡è¶³é«˜è®¡ç®—é‡çš„è¦æ±‚ï¼Œåœ¨ä½è®¡ç®—æ—¶æœŸï¼Œé™ä½è®¡ç®—èŠ‚ç‚¹çš„æ•°é‡èŠ‚çº¦æˆæœ¬ã€‚
    - å­˜å‚¨èŠ‚ç‚¹ä¹Ÿå¯ä»¥æŒ‰éœ€æ‹“å±•ï¼Œä¸¤è€…äº’ç›¸ä¸å†²çªä¸å½±å“ï¼Œå®Œå…¨ç‹¬ç«‹ã€‚è¿™æ ·æ‘†è„±äº†Hadoopæœºå™¨çš„å­˜å‚¨å®¹é‡æˆ–è€…è®¡ç®—èƒ½åŠ›çš„ç“¶é¢ˆã€‚
2. è¿ç»´æˆæœ¬çš„ä¸‹é™
    
    <p align="center">
        <img src="./image/blog1-5.png" alt="image" width="40%">
    </p>

    - Hadoop HDFS éœ€è¦ **æ‰‹åŠ¨ç®¡ç† NameNodeã€DataNodeã€ç£ç›˜æ‰©å®¹ã€æ•°æ®å‰¯æœ¬ç­–ç•¥**ï¼Œç»´æŠ¤å¤æ‚ã€‚å¦‚æœå­˜å‚¨ä¸å¤Ÿï¼Œéœ€è¦å¢åŠ  Hadoop Worker æœºå™¨ï¼ŒåŒæ—¶å¢åŠ ç£ç›˜ã€‚HDFS NameNode éœ€è¦ HAï¼ˆé«˜å¯ç”¨ï¼‰é…ç½®ï¼Œå¦åˆ™ NameNode æŒ‚äº†ï¼Œæ•´ä¸ªå­˜å‚¨å°±æ— æ³•è®¿é—®ã€‚
    - è€Œé€šè¿‡äº‘å­˜å‚¨çš„è¿ç»´å¾ˆç®€å•ï¼š
        - å­˜å‚¨å±‚ä¾‹å¦‚AWS S3ï¼Œç”±å¯¹è±¡å­˜å‚¨è´Ÿè´£ï¼Œæ— éœ€è‡ªå·±ç»´æŠ¤ HDFS NameNode / DataNodeã€‚
        - **äº‘å­˜å‚¨å¯ä»¥è‡ªåŠ¨æ‰©å±•**ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ æ–°æœºå™¨ï¼Œå­˜å‚¨å¯ä»¥æ— é™å¢é•¿ã€‚
        - **æ•°æ®æŒä¹…åŒ–æ›´ç®€å•ï¼š**è®¡ç®—ä»»åŠ¡ç»“æŸåï¼Œå­˜å‚¨ä»ç„¶å¯ç”¨ï¼Œè€Œ Hadoop å¯èƒ½éœ€è¦æ‰‹åŠ¨ç»´æŠ¤ HDFS æ¸…ç†ã€‚
3. æ‘†è„±å¯¹Yarnçš„ä¾èµ–ï¼Œç”±Kubernetesæ¥ç®¡ç†èµ„æº
    
    <p align="center">
        <img src="./image/blog1-6.png" alt="image" width="40%">
    </p>
    
    - åœ¨ Hadoop ç”Ÿæ€ä¸­ï¼Œä¸»è¦è®¡ç®—æ¡†æ¶æ˜¯ **Spark / Flink / Hive**ï¼Œå¹¶ä¸”å®ƒä»¬éƒ½ä¾èµ– YARN ä½œä¸ºè°ƒåº¦å™¨ã€‚è®¡ç®—æ¡†æ¶å—åˆ° Hadoop YARN é™åˆ¶ï¼Œä¸æ–¹ä¾¿åœ¨ Kubernetesã€Serverless ç­‰ç¯å¢ƒä¸­è¿è¡Œã€‚
    - è€Œå­˜ç®—åˆ†ç¦»çš„æ¶æ„ï¼Œ**è®¡ç®—å±‚å¯ä»¥è¿è¡Œåœ¨ Kubernetesï¼Œæ”¯æŒæ›´å¤šè®¡ç®—æ¡†æ¶ï¼Œæ¯”å¦‚Spark on K8Sï¼ŒFlink on K8Sï¼Œé€šè¿‡K8Sæ¥è´Ÿè´£å¯¹è®¡ç®—èµ„æºçš„ä¼¸ç¼©ã€‚**
4. æ›´é€‚åˆäº‘è®¡ç®—ï¼Œæ”¯æŒè·¨åŒºåŸŸéƒ¨ç½²
    - Hadoop YARN å—ç‰©ç†æ•°æ®ä¸­å¿ƒé™åˆ¶
        - è®¡ç®—ä¸Šï¼šä¼ ç»Ÿ Hadoop é›†ç¾¤çš„è®¡ç®—ä»»åŠ¡ **åªèƒ½åœ¨æœ¬åœ°æ•°æ®ä¸­å¿ƒçš„ YARN ä¸Šè¿è¡Œ**ï¼Œéš¾ä»¥è·¨æ•°æ®ä¸­å¿ƒæˆ–å¤šäº‘éƒ¨ç½²ã€‚
        - å­˜å‚¨ä¸Šï¼šå¦‚æœä½ è¦å°† Hadoop æ•°æ®è¿ç§»åˆ°å¦ä¸€ä¸ªæ•°æ®ä¸­å¿ƒï¼Œéœ€è¦æ‰‹åŠ¨åŒæ­¥ HDFS æ•°æ®ã€‚
    - **å­˜ç®—åˆ†ç¦»æ¶æ„**
        - **å­˜å‚¨ä¸Šï¼š**å¯¹è±¡å­˜å‚¨ï¼ˆS3 / MinIO / Cephï¼‰å¯ä»¥éƒ¨ç½²åœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒï¼Œç”šè‡³è·¨äº‘ä½¿ç”¨ã€‚å¤šä¸ªè®¡ç®—é›†ç¾¤å¯ä»¥è®¿é—®åŒä¸€ä¸ª S3 å­˜å‚¨ï¼Œè€Œä¸éœ€è¦ HDFS å¤åˆ¶ã€‚
        - **è®¡ç®—ä¸Šï¼š**è®¡ç®—é›†ç¾¤å¯ä»¥éƒ¨ç½²åœ¨ Kubernetes äº‘ç¯å¢ƒï¼Œéšæ—¶å¼¹æ€§æ‰©å±•
5. æ•´ä½“æˆæœ¬
    
    è®¡ç®—ä»»åŠ¡å®Œæˆåï¼Œè®¡ç®—èµ„æºç«‹å³é‡Šæ”¾ï¼ŒèŠ‚çœæˆæœ¬ï¼ŒHadoopåˆ™éœ€è¦é•¿æœŸç»´æŒæ•´ä¸ªé›†ç¾¤çš„è¿è¡Œã€‚
    
    å­˜å‚¨å±‚å¯ä»¥ä½¿ç”¨ä½æˆæœ¬çš„äº‘å­˜å‚¨ï¼Œè€Œä¸éœ€è¦ç»´æŠ¤æ˜‚è´µçš„HDFSæœåŠ¡å™¨
    
    äº‘è®¡ç®—æ•´ä½“æŒ‰éœ€è®¡è´¹ï¼Œæ‰€ä»¥èµ„æºåˆ©ç”¨ç‡æ›´é«˜ï¼
    

# ğŸš€Â Kubernetesåœ¨äº‘æ•°æ®å·¥ç¨‹ä¸Šçš„åº”ç”¨

åœ¨ç°ä»£äº‘è®¡ç®—ç¯å¢ƒä¸­ï¼Œ**Kubernetesï¼ˆK8sï¼‰** å·²æˆä¸ºå®¹å™¨ç¼–æ’çš„æ ‡å‡†ï¼Œå°¤å…¶åœ¨ **äº‘æ•°æ®å·¥ç¨‹ï¼ˆCloud Data Engineeringï¼‰** é¢†åŸŸï¼ŒK8s ä¸ºæ•°æ®å­˜å‚¨ã€å¤„ç†å’Œæµå¼è®¡ç®—æä¾›äº†æå¤§çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

<p align="center">
    <img src="./image/blog1-7.png" alt="image" width="40%">
</p>

## åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨

Kubernetes å…è®¸ç®¡ç†å’Œè¿è¡Œåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œå¦‚ï¼š

- **å¯¹è±¡å­˜å‚¨ï¼ˆS3 å…¼å®¹ï¼‰**ï¼šMinIOã€Ceph
- **åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨**ï¼šHDFS on Kubernetes
- **å…³ç³»å‹æ•°æ®åº“**ï¼šPostgreSQLã€MySQL
- **NoSQL æ•°æ®åº“**ï¼šCassandraã€MongoDBã€Elasticsearch

**ç¤ºä¾‹æ¶æ„ï¼šHDFS on Kubernetes**

åœ¨ K8s é›†ç¾¤ä¸­è¿è¡Œ **HDFS**ï¼Œå¯ä»¥åŠ¨æ€æ‰©å±• DataNode ä»¥å­˜å‚¨ PB çº§æ•°æ®ï¼Œå¹¶ç»“åˆ PVCï¼ˆPersistent Volume Claimï¼‰ç®¡ç†å­˜å‚¨èµ„æºã€‚

## æ‰¹å¤„ç†æ•°æ®è®¡ç®—

<p align="center">
    <img src="./image/blog1-8.png" alt="image" width="40%">
</p>

**Apache Spark on Kubernetes**

- **Spark Driver åœ¨ K8s Master ä¸Šè¿è¡Œ**ï¼Œè´Ÿè´£ä»»åŠ¡è°ƒåº¦ã€‚
- **Spark Executors ä½œä¸º Pods è¿è¡Œ**ï¼Œå¹¶åŠ¨æ€ä¼¸ç¼©ï¼Œæ”¯æŒå¤§è§„æ¨¡è®¡ç®—ã€‚
- **æ•°æ®å­˜å‚¨åœ¨ HDFS / S3**ï¼Œè®¡ç®—ä»»åŠ¡ç›´æ¥ä»å­˜å‚¨ä¸­è¯»å–æ•°æ®ã€‚

è¿™ç§æ–¹æ³•æ— éœ€Hadoop Yarnï¼ŒSpark Executor ç›´æ¥è¿è¡Œåœ¨ K8s Pods ä¸­ã€‚å¹¶ä¸”æ”¯æŒå¼¹æ€§ä¼¸ç¼©ï¼Œè®¡ç®—ä»»åŠ¡å®Œæˆåï¼Œè‡ªåŠ¨é‡Šæ”¾èµ„æºï¼ŒèŠ‚çœè®¡ç®—æˆæœ¬ã€‚

## å®æ—¶æµæ•°æ®å¤„ç†

<p align="center">
    <img src="./image/blog1-9.png" alt="image" width="40%">
</p>


**Flink on Kubernetes**

- Kafka ç”Ÿäº§æ•°æ®æµ
- **Flink åœ¨ K8s é›†ç¾¤ä¸­è¿è¡Œ**ï¼ŒTaskManagers ä½œä¸º Pods æ‰§è¡Œæµè®¡ç®—
- è®¡ç®—ç»“æœå­˜å…¥ NoSQL æˆ–æ•°æ®æ¹–ï¼ˆS3ã€HDFSï¼‰

è¿™ç§æ–¹æ³•ä¿è¯äº†é«˜å¯ç”¨ï¼Œå¦‚æœFlink Podå´©æºƒï¼ŒK8Så°†è‡ªåŠ¨é‡å¯ï¼ŒåŒæ—¶å¯ä»¥æ ¹æ®æ•°æ®æµé‡æ‰©å±•Flinkçš„è®¡ç®—èŠ‚ç‚¹ã€‚

---