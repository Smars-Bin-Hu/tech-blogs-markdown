> When Apache Hadoop first emerged, it quickly became the dominant force in the big data ecosystem. Many companies adopted it as their **primary big data solution**, and an entire ecosystem of tools and technologies‚Äîsuch as Hive and HBase‚Äîgrew around it. However, in recent years, Hadoop has gradually been replaced by newer technologies. What led to this shift? Let's explore the reasons.
> 

# üöÄ¬†Introduction

<p align="center">
    <img src="./image/blog1-1.png" alt="image" width="40%">
</p>


Since the release of **Hadoop 2.x**, it has been widely used as a big data solution across industries. Hadoop consists of three **core components**:

- **HDFS (Hadoop Distributed File System)** ‚Äì Handles storage.
- **YARN (Yet Another Resource Negotiator)** ‚Äì Manages cluster resources.
- **MapReduce** ‚Äì Processes large-scale data using a batch computation model.

Hadoop's strength lies in its **distributed computing approach**, where storage and computation are managed within the same cluster. **DataNodes and NameNodes** work together to store and replicate massive datasets, while **ResourceManager and NodeManager** manage computing resources. **MapReduce follows a "Map & Reduce" paradigm**, enabling large-scale distributed computations.

Initially, Hadoop was the **go-to solution** for big data processing. Its open-source nature led to the development of **a rich ecosystem** of tools, including **Hive, HBase, and others**, forming a complete big data stack.

# üöÄWhy Was Hadoop Replaced?

Hadoop‚Äôs **three core components**‚ÄîHDFS (storage), MapReduce (computation), and YARN (resource management)‚Äîeach have their own limitations. Over time, these shortcomings led to the gradual decline of Hadoop's dominance.

## üî•¬†1. Computation: The Fall of MapReduce

Originally, Hadoop used **MapReduce** as its computing engine. While **Java-based MapReduce** allowed developers to handle complex computation logic, **it required writing a lot of Java code**.

### The Rise of Hive

With the introduction of **Hive**, users could write SQL-like queries instead of Java code. **Hive's Driver and Compiler** translated SQL into MapReduce jobs, significantly improving **developer efficiency**. However, MapReduce had a fundamental performance limitation: 

- **It relied heavily on disk I/O**, leading to **slow processing speeds,**
- as well as even with optimizations like **Tez**, MapReduce still struggled to match modern computation engines.

### **Hive on Spark: Breaking MapReduce‚Äôs Limits**

The introduction of **Apache Spark** marked a major breakthrough:

- **Spark is memory-based** and processes data up to **10 times faster than MapReduce**.
- It provides **rich APIs** (RDD, DataFrame) in Java, Scala, and Python, making it easier for developers to use.
- Hive adapted to Spark‚Äôs speed by allowing users to switch **MapReduce to Spark as the execution engine**.üëâ **This setup became known as "Hive on Spark".**

### **Shark: A Short-Lived Experiment**

Before **Spark SQL**, the **Shark project** allowed users to execute Hive SQL directly on Spark. Shark translated **Hive SQL into Spark RDD operations**, bypassing MapReduce entirely. However, as **Spark SQL** introduced its **Catalyst Optimizer and Tungsten Execution Engine**, Shark became obsolete and was eventually discontinued.

<p align="center">
    <img src="./image/blog1-2.png" alt="image" width="40%">
</p>

### The Shift to 100% Spark

Whether using **Hive on Spark** or **Shark**, users were still writing **HiveSQL (HQL)**. The only difference was that the execution engine changed to **Spark**, which offered better performance. However, the framework itself remained an **HQL-based architecture**, mainly designed for **data warehouses**. It supported the **Hive ecosystem**, including **Metastore, UDF, and HDFS storage**, which is why many companies continued using it for a long time.

Over time, companies began to **move away from HiveQL** and fully adopt **Spark**.

With **Spark computing directly**, users could execute data operations using **SparkCore (RDD API) and SparkSQL** without relying on **Hive to parse SQL and convert it into Spark tasks**. This **improved performance significantly** and also **enabled real-time processing (Streaming)**.

As a result, **Spark became the main computation engine for many companies**. By writing **SparkSQL**, they could analyze and process large-scale data more efficiently.

Many companies adopted a **new technology stack: HDFS + Spark + YARN**:

- **Data was still stored in HDFS within a Hadoop cluster.**
- **YARN managed computing resources using its ResourceManager.**
- **A dedicated Spark client submitted Spark jobs**, compiled Spark code, and sent the tasks to **YARN for execution**.

The only difference was that

**Spark replaced MapReduce as the execution engine.** üëâ**This is what we call the "Spark on YARN" architecture.**

## üî• **2. Storage Layer**

## **HDFS Core Weaknesses**

| Weakness | Issue |
| --- | --- |
| **Small Files Problem** | HDFS is optimized for storing large files (e.g., GB-level logs) but struggles with many small files (e.g., KB-sized data). Since NameNode stores metadata for each file, too many small files put excessive pressure on it. |
| **Poor Random Read/Write Performance** | HDFS is designed for **large-scale sequential reads/writes** but is **not suitable for low-latency random access**, making it inefficient for database transactions or NoSQL workloads. |
| **High Storage Cost** | HDFS stores **three copies** of each data block by default, using **three times the storage space**, increasing storage costs. |
| **Single Point of Failure (NameNode Dependency)** | NameNode manages metadata. If NameNode fails, the entire HDFS may become unavailable (although HA mode reduces this risk). |
| **Limited Data Updates** | HDFS primarily supports **append-only writes**, meaning files **cannot be modified** (only rewritten entirely), making it unsuitable for **OLTP workloads**. |
| **Not Suitable for Low-Latency Queries** | HDFS requires **MapReduce or Spark** for data processing, leading to **high query latency**. It is not designed for interactive database-style queries. |
| **Lack of Scalability** | Since **storage and computing are coupled**, expanding HDFS requires **adding new Hadoop nodes**, rather than just increasing storage capacity. This increases **operational costs**. |

## **The Architecture of Compute-Storage Coupling**

In this architecture, the **Hadoop cluster** handles **HDFS storage**, while **Spark submits jobs to YARN** for computation. **Essentially, this means compute and storage remain tightly coupled within the same cluster.**

### **Spark on YARN: Job Execution Process**

<p align="center">
    <img src="./image/blog1-3.png" alt="image" width="40%">
</p>

1. **Spark Client Node submits a Spark job to YARN** (requesting resources from YARN).
2. **YARN ResourceManager schedules computing resources**:
    - It starts **ApplicationMaster**, which manages the job lifecycle.
    - It allocates **Executors** on **YARN NodeManagers**, which execute Spark tasks.
3. **Executors process the Spark tasks** and return results to the **Driver on the Spark Client Node**.
4. Since data is stored in **HDFS DataNodes**, Executors must **fetch data from these storage nodes** during computation.

**How Compute-Storage Coupling Works in Hadoop**

- **Each Worker Node** handles both **storage (HDFS DataNode)** and **computation (NodeManager + Executor)**.
- The **Master Node** (ResourceManager + NameNode) centrally manages **both compute resources and storage**.

**That being said, every compute unit and storage unit are coupling in one worker node. Hadoop‚Äôs Data Locality optimization** ensures tasks run on nodes where data is stored, reducing network transfer. However, this architecture introduces several **limitations** in large-scale workloads.

## ‚ùå **Problems with Compute-Storage Coupling**

1. **Compute and storage cannot scale independently**:
    - **Storage scaling**: When data grows, new nodes must be added, even if more compute power isn‚Äôt needed. This leads to **resource waste**.
    - **Compute scaling**: When computational demand increases but storage does not, **scaling the entire cluster is required**, increasing costs.
    - **High maintenance overhead**: Expanding Worker Nodes in Hadoop requires **manual configuration**, reducing **flexibility in scaling**.
2. **Compute and storage compete for resources**:
    - Compute tasks **must wait for storage node resources**, causing **processing delays**.
    - During peak loads (e.g., **AI training, large-scale data analysis**), **storage nodes can become bottlenecks**.
    - Since a **single server handles both HDFS storage and Spark execution**, it can lead to:
        - **I/O contention** (disk read/write competition).
        - **Insufficient memory** (data storage vs. Spark execution).
        - **CPU overload** (both computation and storage operations running simultaneously).
3. **Inefficient hardware utilization**:
    - **Unbalanced compute workload**: Some nodes are heavily loaded, while others remain idle.
    - **Uneven storage load**: HDFS follows a **three-replica strategy**, sometimes leading to **unequal data distribution**, increasing storage pressure on some nodes.

# üöÄ **Storage-Compute Separation**

The **Storage-Compute Separation** architecture **decouples storage from computation**, meaning computing tasks no longer depend on storage nodes. **Data storage is managed by an independent distributed storage cluster** (e.g., AWS S3), while **computation frameworks like Spark and Flink read data remotely to perform processing**.

<p align="center">
    <img src="./image/blog1-4.png" alt="image" width="40%">
</p>

## ‚úÖ **Advantages of Storage-Compute Separation**

### **1. Independent Scaling of Storage and Compute**

**Compute nodes can scale up or down as needed**:

- During peak demand, more compute resources can be added quickly.
- During low demand, compute resources can be reduced to save costs.

**Storage nodes can also scale independently** without affecting compute resources.

This eliminates the **storage or compute bottleneck found in traditional Hadoop systems**.

### **2. Lower Operational Costs**

<p align="center">
    <img src="./image/blog1-5.png" alt="image" width="40%">
</p>

**Hadoop HDFS requires manual management** of NameNode, DataNode, storage expansion, and replication policies. If storage is insufficient, **new Hadoop Worker nodes must be added along with disks**. If NameNode fails, **HDFS may become unavailable unless HA (High Availability) is configured**.

**Cloud storage makes operations much simpler**:

- **For example, AWS S3 automatically handles storage management**‚Äîno need to maintain NameNode/DataNode manually.
- **Cloud storage can scale automatically**, eliminating the need to manually add machines.
- **Data remains available even after compute tasks finish**, whereas HDFS may require manual cleanup.

### 3. Replacing YARN with Kubernetes

<p align="center">
    <img src="./image/blog1-6.png" alt="image" width="40%">
</p>

In Hadoop ecosystems, compute frameworks like **Spark, Flink, and Hive** depend on **YARN** as the resource manager. This limits flexibility, making it difficult to integrate with **Kubernetes and Serverless environments**.

**With storage-compute separation, Kubernetes can manage compute resources**:

- **Run Spark on Kubernetes (Spark on K8s) instead of YARN**.
- **Run Flink on Kubernetes (Flink on K8s) for dynamic scaling**.
- **Kubernetes automatically handles scaling, failure recovery, and resource allocation**.

### **4. More Suitable for Cloud Computing & Multi-Region Deployment**

**Hadoop YARN is limited to a single data center**:

- Compute tasks **must run within the same data center** where YARN is deployed.
- Data migration to another data center requires **manual synchronization of HDFS data**.

**With storage-compute separation**:

- **Storage**: Object storage like **S3 / MinIO / Ceph** can be deployed across multiple regions and clouds.
- **Compute**: Compute clusters can be deployed **on Kubernetes**, allowing **on-demand scaling in any cloud**.

### **5. Cost Efficiency**

**Compute resources are released immediately after tasks finish**, reducing costs.

**Storage uses cost-effective cloud storage** instead of maintaining expensive HDFS clusters.

**Cloud-based pay-as-you-go pricing maximizes resource utilization**.

# üöÄ **Kubernetes in Cloud Data Engineering**

In modern cloud environments, **Kubernetes (K8s)** has become the standard for container orchestration.

It provides **high flexibility and scalability** for **data storage, processing, and streaming analytics**.

<p align="center">
    <img src="./image/blog1-7.png" alt="image" width="40%">
</p>

## **Distributed Data Storage**

Kubernetes allows managing and running **distributed storage systems**, such as:

- **Object storage (S3-compatible)**: MinIO, Ceph
- **Distributed file storage**: HDFS on Kubernetes
- **Relational databases**: PostgreSQL, MySQL
- **NoSQL databases**: Cassandra, MongoDB, Elasticsearch

### üìç¬†**Example: HDFS on Kubernetes**

Running **HDFS on a Kubernetes cluster** allows **dynamic scaling of DataNodes** to store **petabytes (PB) of data**. **Persistent Volume Claims (PVC)** help manage storage resources efficiently.

## Batch Data Processing

<p align="center">
    <img src="./image/blog1-8.png" alt="image" width="40%">
</p>

### üìç¬†**Example: Apache Spark on Kubernetes**

- **Spark Driver runs on the K8s Master node**, handling job scheduling.
- **Spark Executors run as Pods**, dynamically scaling based on demand.
- **Data is stored in HDFS or S3**, and Spark jobs read directly from storage.

This approach **removes the need for Hadoop YARN**. **Spark Executors run directly on Kubernetes Pods**, allowing **elastic scaling**. **When a job finishes, compute resources are released automatically**, reducing costs.

## Real-Time Stream Processing

<p align="center">
    <img src="./image/blog1-9.png" alt="image" width="40%">
</p>

### üìç¬†**Example: Flink on Kubernetes**

- **Kafka produces real-time data streams**.
- **Flink runs on Kubernetes**, with **TaskManagers deployed as Pods** to process streaming data.
- **Results are stored in NoSQL databases or data lakes (S3, HDFS)**.

This setup ensures **high availability**: If a **Flink Pod crashes, Kubernetes automatically restarts it**. Flink can **scale dynamically based on real-time data flow**, improving efficiency.

---

# ÂéªHadoopÂåñÔºöÂ≠òÁÆóÂàÜÁ¶ªÂíåÂü∫‰∫éKubernetesÁöÑ‰∫ëÁ´ØÊï∞ÊçÆÂ∑•Á®ãÁöÑÂÆûË∑µ

> Apache Hadoop‰ªéÂΩìÊó∂ÁöÑ‰∏ÄÂÆ∂Áã¨Â§ßÔºåËÆ©Â§ßÊï∞ÊçÆÁöÑÁîüÊÄÅÂõ¥ÁªïÂú®Hadoop‰∏äÂ±ïÂºÄÔºåÊàê‰∏∫Âá†‰πéÊâÄÊúâÂÖ¨Âè∏ÁöÑÂ§ßÊï∞ÊçÆ‰∏ªÊµÅËß£ÂÜ≥ÊñπÊ°à„ÄÇÂà∞Â¶Ç‰ªäÁöÑÈÄêÊ∏êË¢´ÊõøÊç¢ÔºåÂà∞Â∫ïÂèëÁîü‰∫Ü‰ªÄ‰πà‰∫ãÊÉÖÔºüË∞àË∞àÊàëÁöÑÁêÜËß£„ÄÇ
> 

# üöÄ¬†Introduction

<p align="center">
    <img src="./image/blog1-1.png" alt="image" width="40%">
</p>

Ëá™‰ªéHadoop 2.xÈóÆ‰∏ñÔºåÂÆÉÂ∑≤Áªè‰Ωú‰∏∫ÂæàÂ§öÂÖ¨Âè∏ÁöÑÂ§ßÊï∞ÊçÆËß£ÂÜ≥ÊñπÊ≥ï„ÄÇHadoopÊúâ‰∏â‰∏™Ê†∏ÂøÉÁªÑ‰ª∂Ôºö

- HDFSË¥üË¥£Â≠òÂÇ®
- YarnË¥üË¥£ËµÑÊ∫êÁÆ°ÁêÜ
- MapReduceË¥üË¥£ËÆ°ÁÆó

HadoopÁöÑ‰∏Ä‰∏™ÁâπÁÇπÂ∞±ÊòØÂà©Áî®‰∫ÜÂàÜÂ∏ÉÂºèÁöÑÊÄùÊÉ≥ÔºåËÆ©Â≠òÂÇ®ËµÑÊ∫êÂíåËÆ°ÁÆóËµÑÊ∫êÂú®‰∏Ä‰∏™ÈõÜÁæ§‰∏ãÁªü‰∏ÄÁÆ°ÁêÜÔºåÈÄöËøáDataNodeÂíåNameNode‰πãÈó¥Â∑•‰ΩúÔºåÊù•ÂÆûÁé∞Êµ∑ÈáèÊï∞ÊçÆÁöÑÂ≠òÂÇ®„ÄÅÂ§á‰ªΩÔºå‰ª•ÂèäÈÄöËøáResourceManagerÂíåNodeManagerÔºåÊù•ÁÆ°ÁêÜÂàÜÂ∏ÉÂºèÁöÑÊï∞ÊçÆËÆ°ÁÆóËµÑÊ∫êÔºåÂü∫‰∫éMapÂíåReduceÁöÑÂàÜÊ≤ªÔºàDivide and ConquerÔºâÁÆóÊ≥ïÊÄùÊÉ≥ÔºåÂØπÊµ∑ÈáèÊï∞ÊçÆËøõË°åËÆ°ÁÆó„ÄÇ

Âú®Êó©ÊúüÔºåÈöèÁùÄHadoopÊ°ÜÊû∂ÁöÑÂºÄÊ∫êÔºåÂá†‰πéÊòØÂ§ßÈÉ®ÂàÜÂÖ¨Âè∏Âú®Â§ßÊï∞ÊçÆÈ¢ÜÂüüÁöÑÂîØ‰∏ÄÂåñËß£ÂÜ≥ÊñπÊ°àÔºåÂπ∂‰∏îË°çÁîüÂá∫Êù•‰∫ÜÂæàÂ§öÂæàÂ§öÊñ∞ÁöÑÊäÄÊúØÔºåÊØîÂ¶ÇHiveÔºåHBaseÁ≠âHadoopÁîüÊÄÅ‰∏ãÁöÑÊµÅË°åÈ°∂Á∫ßÈ°πÁõÆÔºåÂÖ±ÂêåÁªÑÊàê‰∫ÜÂ§ßÊï∞ÊçÆÁöÑ‰∏ÄÂ•óÊàêÁÜüÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

# üöÄ¬†HadoopÁöÑÂºäÁ´Ø

HadoopÁöÑ‰∏âÂ§ßÁªÑ‰ª∂ÔºöHDFSË¥üË¥£Â≠òÂÇ®„ÄÅMapReduceË¥üË¥£ËÆ°ÁÆó„ÄÅYarnË¥üË¥£ËµÑÊ∫êÁÆ°ÁêÜÔºå‰ªñ‰ª¨Êúâ‰ªÄ‰πàÁº∫ÁÇπÔºå‰∏∫‰ªÄ‰πàÊúÄÂêéË¢´ÈÄêÊ∏êÂèñ‰ª£‰∫ÜÔºü

## üî•¬†1. ËÆ°ÁÆóÂ±ÇÔºöMapReduceÁöÑÈô®ËêΩ

ÂéüÂÖàHadoopÊòØÂü∫‰∫éMapReduceËøõË°åËÆ°ÁÆóÔºåMapReduceÂØπJavaÁöÑÊîØÊåÅÔºå‰ΩøÂæóÂèØ‰ª•Ëß£ÂÜ≥ÂæàÂ§öÂæàÂ§çÊùÇÁöÑËÆ°ÁÆóÈÄªËæëÔºå‰ΩÜÊòØÁî®Êà∑ËøòÊòØÈúÄË¶ÅÊâãÂÜôÂ§ßÈáèÁöÑJava‰ª£Á†Å„ÄÇ

### HiveÁöÑÂÖ¥Ëµ∑

ÈöèÁùÄHiveÁöÑÂá∫Áé∞ÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáSQLÊù•ÂÆûÁé∞Â§çÊùÇÈÄªËæëÁöÑËÆ°ÁÆóÔºåHive DriverÂíåHive CompilerÁ≠âÁªÑ‰ª∂Ë¥üË¥£Ëß£ÊûêÁî®Êà∑Êèê‰∫§ÁöÑSQLËØ≠Âè•Âπ∂‰∏îËΩ¨Êç¢‰∏∫MapReduce‰ªªÂä°ÔºåÂ§ßÂ§ßÊèêÂçá‰∫ÜÂºÄÂèëËÄÖÁöÑÂºÄÂèëÊïàÁéá„ÄÇ‰ΩÜÊòØMapReduceÂõ†‰∏∫ÊòØÂü∫‰∫éÁ£ÅÁõòËÆ°ÁÆóÁöÑÔºåÂÖ∂ÊÄßËÉΩÈùûÂ∏∏ÊúâÈôê„ÄÇÂç≥‰ΩøÈöèÁùÄTezËÆ°ÁÆóÂºïÊìéÁöÑÂá∫Áé∞ÔºåËÆ°ÁÆóÊÄßËÉΩ‰∏äÊúâÊâÄÊèêÂçáÔºå‰ªçÁÑ∂Êï¥‰ΩìÂ∑ÆÂº∫‰∫∫ÊÑè„ÄÇ

### Hive on Spark

SparkÁöÑÂá∫Áé∞ÔºåÂØπMapReduceËøõË°å‰∫ÜÁ™ÅÁ†¥„ÄÇSparkÊòØÂü∫‰∫éÂÜÖÂ≠òËÆ°ÁÆóÔºåÂπ∂‰∏îÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑRDDÁÆóÂ≠êËÆ°ÁÆóÔºåËÆ©Java/Scale/PythonÂºÄÂèëËÄÖÔºåÂèØ‰ª•ÂÜôRDDÂáΩÊï∞ÊàñËÄÖDSLÂáΩÊï∞ÔºåÊù•ÂÆåÊàêÂØπSparkÁöÑÂºÄÂèëÔºåÂπ∂‰∏îÈÄöËøáÂÜÖÂ≠òËÆ°ÁÆóÔºåËøë100ÂÄç‰∫é‰πãÂâçMapReduceÁöÑÊâßË°åÊïàÁéá„ÄÇ

HiveÂΩìÊó∂‰Ωú‰∏∫Â§ßÊï∞ÊçÆÁöÑ‰∏Ä‰∏™‰∏ªÊµÅÊï∞‰ªìÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÂéüÊú¨ÊòØÈÄöËøáMetastoreÂ≠òÂÇ®ÂÖÉÊï∞ÊçÆÔºåÊù•ËØªÂèñHDFS‰∏äÁöÑÊï∞ÊçÆÔºåÈÄöËøáSQLÁºñËØëÂô®Êù•ÊâßË°åMapReduce‰ªªÂä°ÔºåÂêéÊù•Ôºå**HiveÈÄöËøáÂØπËÆ°ÁÆóÂºïÊìéÁöÑÈÖçÁΩÆÔºåËÆ©ËÆ°ÁÆóÈÄöËøáSparkÊù•ËøõË°åÔºåËÄå‰∏çÊòØMapReduceÊàñËÄÖTez„ÄÇËøôÂ∞±ÊòØÊàë‰ª¨Â∏∏ËØ¥ÁöÑHive on Spark„ÄÇ**

### SharkÔºàÂ∑≤Ë¢´Ê∑òÊ±∞Ôºâ

Ëøô‰∏™Ê¶ÇÂøµÔºåÂèØËÉΩÂ§ßÈÉ®ÂàÜËµÑÊ∑±ÁöÑData EngineerÈÉΩÂê¨ËØ¥ËøáÔºåÊòØSparkÁ§æÂå∫Êó©ÊúüÁöÑ‰∏Ä‰∏™È°πÁõÆÔºåÁõÆÁöÑÊòØËÆ©Áî®Êà∑Êèê‰∫§Hive SQLÂú®Spark‰∏äËøêË°åÔºàÁî®Êà∑ÂÜô Hive SQLÔºåShark Ëß£ÊûêÂπ∂ËΩ¨Êç¢‰∏∫ Spark RDD ‰ªªÂä°Ôºâ‰ΩÜÊòØÂêéÊù•Âá∫Áé∞‰∫ÜSparkSQLÔºåËøô‰∏™È°πÁõÆÂ∞±ÂÅúÊ≠¢‰∫Ü„ÄÇ

<p align="center">
    <img src="./image/blog1-2.png" alt="image" width="40%">
</p>

### The Shift to 100% Spark

Hive on SparkÊàñËÄÖShark‰πüÂ•ΩÔºåÁî®Êà∑‰ªçÁÑ∂ÂÜôÁöÑÊòØHiveSQLÔºàHQLÔºâÔºåÂè™ÊòØ‰ªªÂä°Êèê‰∫§Âà∞SparkÊâßË°åÂºïÊìéËÄåÂ∑≤ÔºåÁî±ÊÄßËÉΩÊõ¥Âº∫ÁöÑSparkËøõË°åËÆ°ÁÆó„ÄÇ‰ΩÜÊòØÊï¥‰∏™Ê°ÜÊû∂‰ªçÁÑ∂ËøòÊòØHQLÊ°ÜÊû∂ÔºåËøôÂ•óÊ°ÜÊû∂‰∏ªË¶ÅÊòØÈÄÇÁî®‰∫éÊï∞ÊçÆ‰ªìÂ∫ìÂú∫ÊôØÔºåÊîØÊåÅhiveÁîüÊÄÅÔºåÂ¶ÇMetastore„ÄÅUDF„ÄÅHDFSÂ≠òÂÇ®ÔºåÊâÄ‰ª•ËÉΩ‰∏ÄÁõ¥Âú®ÂæàÂ§öÂÖ¨Âè∏Ê≤øÁî®Âà∞Áé∞Âú®„ÄÇ‰ΩÜÊòØÊ∏êÊ∏êÂú∞Ôºå‰∫∫‰ª¨ÂºÄÂßãÊ∑òÊ±∞HiveQLÁöÑÊ°ÜÊû∂ÔºåËΩ¨ËÄå100%ÊäïÂÖ•Âà∞SparkÁöÑÊÄÄÊä±„ÄÇ

ËÄåSparkÁõ¥Êé•ËÆ°ÁÆóÔºåÊòØÁî®Êà∑Áõ¥Êé•ÈÄöËøáSparkCoreÔºàRDD APIÔºâ Âíå SparkSQLÁõ¥Êé•Êù•ÊâßË°åÊï∞ÊçÆÁöÑËÆ°ÁÆó„ÄÇËøôÊ†∑ÊÄßËÉΩ‰ºöÊõ¥Âä†ÊèêÈ´òÔºåÂõ†‰∏∫ÁúÅÂéª‰∫ÜHiveËß£ÊûêSQLÂíåÁîüÊàêSpark‰ªªÂä°ÁöÑÁéØËäÇÔºåÂêåÊó∂‰πüÊîØÊåÅÂÆûÊó∂ËÆ°ÁÆóÔºàStreamingÔºâ„ÄÇ

Ëá≥Ê≠§ÔºåSparkÈÄêÊ∏êÊàê‰∏∫‰∫ÜÂæàÂ§öÂÖ¨Âè∏ÁöÑ‰∏ªÊµÅËÆ°ÁÆóÂºïÊìéÔºåÂ§ßÂÆ∂ÈÄöËøáÁõ¥Êé•ÂÜôSparkSQLÔºåÂ∞±ÂÆûÁé∞‰∫ÜÂØπÊµ∑ÈáèÊï∞ÊçÆÁöÑÂàÜÊûêËÆ°ÁÆó„ÄÇÂõ†Ê≠§ÂêéÈù¢ÂæàÂ§öÂÖ¨Âè∏ÁöÑ‰∏Ä‰∏™ÊäÄÊúØÊû∂ÊûÑÊòØHDFS+Spark+YarnÔºöÊï¥‰ΩìÊï∞ÊçÆ‰ªçÁÑ∂ÊòØÂ≠òÂú®HadoopÈõÜÁæ§‰∏äÁöÑHDFSÔºåËµÑÊ∫êÁÆ°ÁêÜ‰πüÊòØÂú®HadoopÈõÜÁæ§‰∏äÂü∫‰∫éYarnÁöÑResourceManagerÊù•ÂÆûÁé∞Ôºå‰ΩÜÊòØÊúâ‰∏Ä‰∏™‰∏ìÈó®ÁöÑSparkÂÆ¢Êà∑Á´ØÔºåË¥üË¥£Êèê‰∫§Spark JobÔºåËß£ÊûêSpark‰ª£Á†ÅÂπ∂‰∏îÊèê‰∫§ÁªôHadoopÈõÜÁæ§ÁöÑYarnÊù•ËøõË°åËÆ°ÁÆóÔºåÂè™ÊòØËÆ°ÁÆóÂºïÊìéÊòØSparkËÄåÂ∑≤„ÄÇ**ËøôÂ∞±ÊòØÊàë‰ª¨Â∏∏ËØ¥ÁöÑSpark on YarnÁöÑÊû∂ÊûÑ„ÄÇ**

## üî•¬†‰∫å„ÄÅÂ≠òÂÇ®Â±ÇÈù¢

## HDFSÊ†∏ÂøÉÂä£Âäø

| Âä£Âäø | ÂÖ∑‰ΩìÈóÆÈ¢ò |
| --- | --- |
| **Â∞èÊñá‰ª∂ÈóÆÈ¢òÔºàSmall Files IssueÔºâ** | HDFS ÈÄÇÁî®‰∫éÂ≠òÂÇ®Â§ßÊñá‰ª∂ÔºàÂ¶Ç GB Á∫ßÊó•ÂøóÔºâÔºå‰ΩÜ‰∏çÈÄÇÂêàÂ≠òÂÇ®Â§ßÈáèÂ∞èÊñá‰ª∂ÔºàÂ¶Ç KB Á∫ßÊï∞ÊçÆÔºâÔºåÂõ†‰∏∫ NameNode ÈúÄË¶ÅÂ≠òÂÇ®Êñá‰ª∂ÂÖÉÊï∞ÊçÆÔºåÂØºËá¥ÂÖÉÊï∞ÊçÆÂéãÂäõËøáÂ§ß„ÄÇ |
| ‰ΩéÈöèÊú∫ËØªÂÜôÊÄßËÉΩ | HDFS ÈÄÇÁî®‰∫é**Â§ßËßÑÊ®°È°∫Â∫èËØªÂÜô**Ôºå‰∏çÈÄÇÂêà**‰ΩéÂª∂ËøüÈöèÊú∫ËÆøÈóÆ**ÔºåÂõ†Ê≠§Êó†Ê≥ïÈ´òÊïàÊîØÊåÅÊï∞ÊçÆÂ∫ì‰∫ãÂä°Êàñ NoSQL ‰∏öÂä°„ÄÇ |
| ‰∏âÂâØÊú¨Â≠òÂÇ®ÊàêÊú¨È´ò | ÈªòËÆ§ÊØè‰∏™Êï∞ÊçÆÂùóÂ≠ò 3 ‰ªΩÔºåÂç†Áî® 3 ÂÄçÂ≠òÂÇ®Á©∫Èó¥ÔºåÂ≠òÂÇ®ÊàêÊú¨ËæÉÈ´ò„ÄÇ |
| ÂçïÁÇπÊïÖÈöúÔºàNameNode ‰æùËµñÔºâ | NameNode Â≠òÂÇ®ÂÖÉÊï∞ÊçÆÔºåÂ¶ÇÊûú NameNode ÊïÖÈöúÔºåÊï¥‰∏™ HDFS ÂèØËÉΩ‰∏çÂèØÁî®ÔºàÂ∞ΩÁÆ° HA Ê®°ÂºèÂèØÁºìËß£Ôºâ„ÄÇ |
| Êï∞ÊçÆÊõ¥Êñ∞‰∏çÁÅµÊ¥ª | HDFS ‰∏ªË¶ÅÊîØÊåÅ **ËøΩÂä†ÂÜôÔºàAppend OnlyÔºâ**Ôºå‰∏çÊîØÊåÅÊñá‰ª∂‰øÆÊîπÔºàÂè™ËÉΩÈáçÂÜôÊï¥‰∏™Êñá‰ª∂ÔºâÔºå‰∏çÈÄÇÂêà OLTP ‰∏öÂä°„ÄÇ |
| ‰∏çÈÄÇÁî®‰∫é‰ΩéÂª∂ËøüÊü•ËØ¢ | HDFS ÈúÄË¶Å MapReduce/Spark Â§ÑÁêÜÊï∞ÊçÆÔºåËÆ°ÁÆóÂª∂ËøüËæÉÈ´òÔºå‰∏çÈÄÇÁî®‰∫é‰∫§‰∫íÂºèÊü•ËØ¢ÔºàÂ¶ÇÊï∞ÊçÆÂ∫ìÁ∫ßÊü•ËØ¢Ôºâ„ÄÇ |
| Êâ©Â±ï‰∏çÂ§üÁÅµÊ¥ª | ËÆ°ÁÆóÂíåÂ≠òÂÇ®ËÄ¶ÂêàÔºåÊâ©Â±ï HDFS ÈúÄË¶ÅÂ¢ûÂä†Êï¥‰∏™ Hadoop ÊúçÂä°Âô®ÔºåËÄå‰∏çÊòØÂçïÁã¨Êâ©Â±ïÂ≠òÂÇ®„ÄÇËøôÁßçËøêÁª¥ÁöÑÊàêÊú¨Ëä±Ë¥πÊØîËæÉÂ§ö„ÄÇ |

## Â≠òÁÆóËÄ¶ÂêàÁöÑÊû∂ÊûÑ

‰Ω†ÂèØËÉΩÂèëÁé∞‰∫ÜËøôÂ•óÊû∂ÊûÑÁöÑÊ®°ÂºèÔºöHadoopÈõÜÁæ§Ë¥üË¥£HDFSÁöÑÂ≠òÂÇ®ÔºåÂπ∂‰∏îSparkÊèê‰∫§jobÂà∞HadoopÈõÜÁæ§Yarn‰∏äÔºåËøõË°åËÆ°ÁÆóÔºåÂÆûË¥®‰∏äËøô‰ªçÁÑ∂ÊòØ‰∏ÄÂ•óÂ≠òÂÇ®ÂíåËÆ°ÁÆóËÄ¶ÂêàÁöÑÊ°ÜÊû∂„ÄÇ

### **Spark on YarnÁöÑJob ExecutionÊµÅÁ®ã**

<p align="center">
    <img src="./image/blog1-3.png" alt="image" width="40%">
</p>

1. Spark Client NodeÂú®Êèê‰∫§Spark‰ªªÂä°ÁªôYarnÂêéÔºàÂêëYarnËØ∑Ê±ÇËµÑÊ∫êÔºâ
2. Yarn ResourceManagerÂ∞ÜË¥üË¥£Ë∞ÉÂ∫¶ËÆ°ÁÆóËµÑÊ∫êÔºö 
    - ÂêØÂä®ApplicationMasterÂπ∂‰∏îÁî≥ËØ∑ËµÑÊ∫êÔºå
    - Âπ∂Âú®Yarn NodeManager‰∏äÂêØÂä®ExecutorÔºàË¥üË¥£ÊâßË°åSpark‰ªªÂä°ÁöÑËøõÁ®ãÔºâ
3. ExecutorÊâßË°åSparkËÆ°ÁÆó‰ªªÂä°ÔºåÁÑ∂ÂêéÊääÁªìÊûúËøîÂõûÁªôDriver on the Spark Client node„ÄÇ
4. Âú®Ëøô‰∏™Êû∂ÊûÑ‰∏≠ÔºåÊï∞ÊçÆ‰ªçÁÑ∂Â≠òÊîæÂú®HDFS Data NodeÔºåÊâÄ‰ª•Executor‰ªçÁÑ∂Ë¶ÅÈÄöËøáËÆøÈóÆDataNodeÊù•ËØªÂèñÊï∞ÊçÆ„ÄÇ

ÊâÄ‰ª•Âú®HadoopÈõÜÁæ§‰∏äÔºåÊØè‰∏™Worker Node‰ªçÁÑ∂ÊòØÊó¢Ë¥üË¥£Â≠òÂÇ®ÔºàHDFS Data NodeÔºâÔºåÂèàË¥üË¥£ËÆ°ÁÆóÔºàNodeManager + ExecutorÔºâÔºåÂêåÊó∂Áî±MasterNodeÔºàResourceManager + DataNodeÔºâË¥üË¥£Áªü‰∏ÄÁöÑËµÑÊ∫êÂíåÂ≠òÂÇ®ÁÆ°ÁêÜ„ÄÇ

‰πüÂ∞±ÊòØËØ¥ÔºåHadoopÁöÑÊó©ÊúüÊû∂ÊûÑ‰∏≠ÔºåËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊòØÁªëÂÆöÂú®Âêå‰∏ÄÁªÑÊúçÂä°Âô®‰∏äÁöÑÔºåÊØè‰∏™DataNodeÊó¢Â≠òÂÇ®Êï∞ÊçÆÂèàÊâßË°åËÆ°ÁÆó‰ªªÂä°„ÄÇÂ∞ΩÁÆ°HadoopÈÄöËøáData LocalityÊù•‰ºòÂåñËÆ°ÁÆóÔºàËÆ©‰ªªÂä°Â∞ΩÈáèÂú®Â≠òÂÇ®Êï∞ÊçÆÁöÑËäÇÁÇπ‰∏äËøêË°åÔºå‰ª•ÂáèÂ∞ëÁΩëÁªú‰º†ËæìÔºâÔºå‰ΩÜËøô‰πü‰∏çÂèØÈÅøÂÖçÂú∞ÔºåÂú®‰∏Ä‰∫õÂú∫ÊôØ‰∏≠Âá∫Áé∞‰∫ÜÂæàÂ§öÈóÆÈ¢ò„ÄÇ

## ‚ùå¬†Â≠òÁÆóËÄ¶ÂêàÂ∏¶Êù•ÁöÑÂºäÁ´Ø

1. **Â≠òÂÇ®ÂíåËÆ°ÁÆóËµÑÊ∫êÊó†Ê≥ïÁã¨Á´ãÊâ©Â±ï**Ôºö
    - **Â≠òÂÇ®Êâ©Â±ï**ÔºöÂ¶ÇÊûúÊï∞ÊçÆÂ¢ûÈïøÔºåÈúÄË¶ÅÂ¢ûÂä†Êñ∞ÁöÑËäÇÁÇπÔºå‰ΩÜËÆ°ÁÆóËµÑÊ∫êÂèØËÉΩËøáÂâ©ÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥π„ÄÇ
    - **ËÆ°ÁÆóÊâ©Â±ï**ÔºöÂ¶ÇÊûúËÆ°ÁÆó‰ªªÂä°Â¢ûÂä†Ôºå‰ΩÜÂ≠òÂÇ®‰∏çÈúÄË¶ÅÂ¢ûÂä†Ôºå‰ªçÁÑ∂ÈúÄË¶ÅÊâ©ÂÆπÊï¥‰∏™ÈõÜÁæ§ÔºåÊàêÊú¨Â¢ûÂä†„ÄÇ
    - Âπ∂‰∏îHadoopÁöÑNodeÁª¥Êä§ÊàêÊú¨ÊØîËæÉÂ§ßÔºåworknodeÁöÑÊâ©ÂÆπÔºåÈÉΩÈúÄË¶ÅÊâãÂä®ËøêÁª¥ÔºåËÆ©Êï¥‰∏™Êâ©ÂÆπÂíåÁº©ÂÆπÁöÑÁÅµÊ¥ªÊÄß‰∏ãÈôçÂæàÂ§ö„ÄÇ
2. **Â≠òÂÇ®ÂíåËÆ°ÁÆóËµÑÊ∫êÂÜ≤Á™Å**Ôºö
    - ËÆ°ÁÆó‰ªªÂä°ÈúÄË¶ÅÊéíÈòüÁ≠âÂæÖÂ≠òÂÇ®ËäÇÁÇπÁöÑËµÑÊ∫êÔºåÂΩ±ÂìçËÆ°ÁÆóÊïàÁéá„ÄÇ
    - ÂΩì‰ªªÂä°ÈáèÈ´òÂ≥∞Êó∂ÔºàÂ¶Ç AI ËÆ≠ÁªÉ„ÄÅÂ§ßËßÑÊ®°Êï∞ÊçÆÂàÜÊûêÔºâÔºåÂ≠òÂÇ®ËäÇÁÇπÂèØËÉΩÊàê‰∏∫ËÆ°ÁÆóÁì∂È¢à„ÄÇ
    - Âêå‰∏ÄÂè∞ÊúçÂä°Âô®Êó¢Ë¥üË¥£Â≠òÂÇ® HDFS Êï∞ÊçÆÔºåÂèàË¥üË¥£ËÆ°ÁÆó Spark ‰ªªÂä°ÔºåÂèØËÉΩÂØºËá¥ **I/O ‰∫âÁî®„ÄÅÂÜÖÂ≠ò‰∏çË∂≥„ÄÅCPU ËøáËΩΩ** Á≠âÈóÆÈ¢ò„ÄÇ
3. **Á°¨‰ª∂ËµÑÊ∫êÂà©Áî®Áéá‰Ωé**Ôºö
    - **ËÆ°ÁÆóË¥üËΩΩ‰∏çÂùáË°°**ÔºöÈÉ®ÂàÜËäÇÁÇπË¥üËΩΩÈ´òÔºåËÄåÂÖ∂‰ªñËäÇÁÇπÁ©∫Èó≤„ÄÇ
    - **Â≠òÂÇ®Ë¥üËΩΩ‰∏çÂùáË°°**ÔºöHDFS ÈááÁî® **‰∏âÂâØÊú¨Êú∫Âà∂**ÔºåÂØºËá¥Êï∞ÊçÆÂàÜÂ∏ÉÂèØËÉΩ‰∏çÂùáË°°ÔºåÊüê‰∫õËäÇÁÇπÂ≠òÂÇ®ÂéãÂäõÂ§ß„ÄÇ

# üöÄ¬†Â≠òÁÆóÂàÜÁ¶ª

Â≠òÁÆóÂàÜÁ¶ªÔºàStorage-Compute SeparationÔºâÊû∂ÊûÑÂ∞ÜÂ≠òÂÇ®ÂíåËÆ°ÁÆóÊãÜÂàÜÔºåËÆ°ÁÆó‰ªªÂä°‰∏ç‰æùËµñ‰∫éÂ≠òÂÇ®ËäÇÁÇπ„ÄÇÊï∞ÊçÆÂ≠òÂÇ®ËäÇÁÇπÂú®Áã¨Á´ãÁöÑÂàÜÂ∏ÉÂºèÂ≠òÂÇ®ÈõÜÁæ§‰∏äÔºàÂ¶Ç AWS S3ÔºâÔºåËÆ°ÁÆóÊ°ÜÊû∂ËäÇÁÇπÔºàSpark„ÄÅFlinkÔºâ‰ªéËøúÁ®ãÂ≠òÂÇ®ËØªÂèñÊï∞ÊçÆÊâßË°åËÆ°ÁÆó„ÄÇ

<p align="center">
    <img src="./image/blog1-4.png" alt="image" width="40%">
</p>

## ‚úÖÂ≠òÂÇ®ËÆ°ÁÆóÂàÜÊûêÁöÑ‰ºòÂäø

1. Â≠òÂÇ®ÂíåËÆ°ÁÆóÂèØ‰ª•Áã¨Á´ãÊãìÂ±ïÂíåÊî∂Áº©
    - ËÆ°ÁÆóËäÇÁÇπÂèØ‰ª•ÊåâÈúÄÊãìÂ±ïÔºåÊØîÂ¶ÇËÆ°ÁÆóÈúÄÊ±ÇÈ´òÂ≥∞Êó∂ÊúüÔºåÂèØ‰ª•Âø´ÈÄüÂ¢ûÂä†ËÆ°ÁÆóËµÑÊ∫êÊù•Êª°Ë∂≥È´òËÆ°ÁÆóÈáèÁöÑË¶ÅÊ±ÇÔºåÂú®‰ΩéËÆ°ÁÆóÊó∂ÊúüÔºåÈôç‰ΩéËÆ°ÁÆóËäÇÁÇπÁöÑÊï∞ÈáèËäÇÁ∫¶ÊàêÊú¨„ÄÇ
    - Â≠òÂÇ®ËäÇÁÇπ‰πüÂèØ‰ª•ÊåâÈúÄÊãìÂ±ïÔºå‰∏§ËÄÖ‰∫íÁõ∏‰∏çÂÜ≤Á™Å‰∏çÂΩ±ÂìçÔºåÂÆåÂÖ®Áã¨Á´ã„ÄÇËøôÊ†∑ÊëÜËÑ±‰∫ÜHadoopÊú∫Âô®ÁöÑÂ≠òÂÇ®ÂÆπÈáèÊàñËÄÖËÆ°ÁÆóËÉΩÂäõÁöÑÁì∂È¢à„ÄÇ
2. ËøêÁª¥ÊàêÊú¨ÁöÑ‰∏ãÈôç
    
    <p align="center">
        <img src="./image/blog1-5.png" alt="image" width="40%">
    </p>

    - Hadoop HDFS ÈúÄË¶Å **ÊâãÂä®ÁÆ°ÁêÜ NameNode„ÄÅDataNode„ÄÅÁ£ÅÁõòÊâ©ÂÆπ„ÄÅÊï∞ÊçÆÂâØÊú¨Á≠ñÁï•**ÔºåÁª¥Êä§Â§çÊùÇ„ÄÇÂ¶ÇÊûúÂ≠òÂÇ®‰∏çÂ§üÔºåÈúÄË¶ÅÂ¢ûÂä† Hadoop Worker Êú∫Âô®ÔºåÂêåÊó∂Â¢ûÂä†Á£ÅÁõò„ÄÇHDFS NameNode ÈúÄË¶Å HAÔºàÈ´òÂèØÁî®ÔºâÈÖçÁΩÆÔºåÂê¶Âàô NameNode ÊåÇ‰∫ÜÔºåÊï¥‰∏™Â≠òÂÇ®Â∞±Êó†Ê≥ïËÆøÈóÆ„ÄÇ
    - ËÄåÈÄöËøá‰∫ëÂ≠òÂÇ®ÁöÑËøêÁª¥ÂæàÁÆÄÂçïÔºö
        - Â≠òÂÇ®Â±Ç‰æãÂ¶ÇAWS S3ÔºåÁî±ÂØπË±°Â≠òÂÇ®Ë¥üË¥£ÔºåÊó†ÈúÄËá™Â∑±Áª¥Êä§ HDFS NameNode / DataNode„ÄÇ
        - **‰∫ëÂ≠òÂÇ®ÂèØ‰ª•Ëá™Âä®Êâ©Â±ï**Ôºå‰Ω†‰∏çÈúÄË¶ÅÊâãÂä®Ê∑ªÂä†Êñ∞Êú∫Âô®ÔºåÂ≠òÂÇ®ÂèØ‰ª•Êó†ÈôêÂ¢ûÈïø„ÄÇ
        - **Êï∞ÊçÆÊåÅ‰πÖÂåñÊõ¥ÁÆÄÂçïÔºö**ËÆ°ÁÆó‰ªªÂä°ÁªìÊùüÂêéÔºåÂ≠òÂÇ®‰ªçÁÑ∂ÂèØÁî®ÔºåËÄå Hadoop ÂèØËÉΩÈúÄË¶ÅÊâãÂä®Áª¥Êä§ HDFS Ê∏ÖÁêÜ„ÄÇ
3. ÊëÜËÑ±ÂØπYarnÁöÑ‰æùËµñÔºåÁî±KubernetesÊù•ÁÆ°ÁêÜËµÑÊ∫ê
    
    <p align="center">
        <img src="./image/blog1-6.png" alt="image" width="40%">
    </p>
    
    - Âú® Hadoop ÁîüÊÄÅ‰∏≠Ôºå‰∏ªË¶ÅËÆ°ÁÆóÊ°ÜÊû∂ÊòØ **Spark / Flink / Hive**ÔºåÂπ∂‰∏îÂÆÉ‰ª¨ÈÉΩ‰æùËµñ YARN ‰Ωú‰∏∫Ë∞ÉÂ∫¶Âô®„ÄÇËÆ°ÁÆóÊ°ÜÊû∂ÂèóÂà∞ Hadoop YARN ÈôêÂà∂Ôºå‰∏çÊñπ‰æøÂú® Kubernetes„ÄÅServerless Á≠âÁéØÂ¢É‰∏≠ËøêË°å„ÄÇ
    - ËÄåÂ≠òÁÆóÂàÜÁ¶ªÁöÑÊû∂ÊûÑÔºå**ËÆ°ÁÆóÂ±ÇÂèØ‰ª•ËøêË°åÂú® KubernetesÔºåÊîØÊåÅÊõ¥Â§öËÆ°ÁÆóÊ°ÜÊû∂ÔºåÊØîÂ¶ÇSpark on K8SÔºåFlink on K8SÔºåÈÄöËøáK8SÊù•Ë¥üË¥£ÂØπËÆ°ÁÆóËµÑÊ∫êÁöÑ‰º∏Áº©„ÄÇ**
4. Êõ¥ÈÄÇÂêà‰∫ëËÆ°ÁÆóÔºåÊîØÊåÅË∑®Âå∫ÂüüÈÉ®ÁΩ≤
    - Hadoop YARN ÂèóÁâ©ÁêÜÊï∞ÊçÆ‰∏≠ÂøÉÈôêÂà∂
        - ËÆ°ÁÆó‰∏äÔºö‰º†Áªü Hadoop ÈõÜÁæ§ÁöÑËÆ°ÁÆó‰ªªÂä° **Âè™ËÉΩÂú®Êú¨Âú∞Êï∞ÊçÆ‰∏≠ÂøÉÁöÑ YARN ‰∏äËøêË°å**ÔºåÈöæ‰ª•Ë∑®Êï∞ÊçÆ‰∏≠ÂøÉÊàñÂ§ö‰∫ëÈÉ®ÁΩ≤„ÄÇ
        - Â≠òÂÇ®‰∏äÔºöÂ¶ÇÊûú‰Ω†Ë¶ÅÂ∞Ü Hadoop Êï∞ÊçÆËøÅÁßªÂà∞Âè¶‰∏Ä‰∏™Êï∞ÊçÆ‰∏≠ÂøÉÔºåÈúÄË¶ÅÊâãÂä®ÂêåÊ≠• HDFS Êï∞ÊçÆ„ÄÇ
    - **Â≠òÁÆóÂàÜÁ¶ªÊû∂ÊûÑ**
        - **Â≠òÂÇ®‰∏äÔºö**ÂØπË±°Â≠òÂÇ®ÔºàS3 / MinIO / CephÔºâÂèØ‰ª•ÈÉ®ÁΩ≤Âú®Â§ö‰∏™Êï∞ÊçÆ‰∏≠ÂøÉÔºåÁîöËá≥Ë∑®‰∫ë‰ΩøÁî®„ÄÇÂ§ö‰∏™ËÆ°ÁÆóÈõÜÁæ§ÂèØ‰ª•ËÆøÈóÆÂêå‰∏Ä‰∏™ S3 Â≠òÂÇ®ÔºåËÄå‰∏çÈúÄË¶Å HDFS Â§çÂà∂„ÄÇ
        - **ËÆ°ÁÆó‰∏äÔºö**ËÆ°ÁÆóÈõÜÁæ§ÂèØ‰ª•ÈÉ®ÁΩ≤Âú® Kubernetes ‰∫ëÁéØÂ¢ÉÔºåÈöèÊó∂ÂºπÊÄßÊâ©Â±ï
5. Êï¥‰ΩìÊàêÊú¨
    
    ËÆ°ÁÆó‰ªªÂä°ÂÆåÊàêÂêéÔºåËÆ°ÁÆóËµÑÊ∫êÁ´ãÂç≥ÈáäÊîæÔºåËäÇÁúÅÊàêÊú¨ÔºåHadoopÂàôÈúÄË¶ÅÈïøÊúüÁª¥ÊåÅÊï¥‰∏™ÈõÜÁæ§ÁöÑËøêË°å„ÄÇ
    
    Â≠òÂÇ®Â±ÇÂèØ‰ª•‰ΩøÁî®‰ΩéÊàêÊú¨ÁöÑ‰∫ëÂ≠òÂÇ®ÔºåËÄå‰∏çÈúÄË¶ÅÁª¥Êä§ÊòÇË¥µÁöÑHDFSÊúçÂä°Âô®
    
    ‰∫ëËÆ°ÁÆóÊï¥‰ΩìÊåâÈúÄËÆ°Ë¥πÔºåÊâÄ‰ª•ËµÑÊ∫êÂà©Áî®ÁéáÊõ¥È´òÔºÅ
    

# üöÄ¬†KubernetesÂú®‰∫ëÊï∞ÊçÆÂ∑•Á®ã‰∏äÁöÑÂ∫îÁî®

Âú®Áé∞‰ª£‰∫ëËÆ°ÁÆóÁéØÂ¢É‰∏≠Ôºå**KubernetesÔºàK8sÔºâ** Â∑≤Êàê‰∏∫ÂÆπÂô®ÁºñÊéíÁöÑÊ†áÂáÜÔºåÂ∞§ÂÖ∂Âú® **‰∫ëÊï∞ÊçÆÂ∑•Á®ãÔºàCloud Data EngineeringÔºâ** È¢ÜÂüüÔºåK8s ‰∏∫Êï∞ÊçÆÂ≠òÂÇ®„ÄÅÂ§ÑÁêÜÂíåÊµÅÂºèËÆ°ÁÆóÊèê‰æõ‰∫ÜÊûÅÂ§ßÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

<p align="center">
    <img src="./image/blog1-7.png" alt="image" width="40%">
</p>

## ÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂ≠òÂÇ®

Kubernetes ÂÖÅËÆ∏ÁÆ°ÁêÜÂíåËøêË°åÂàÜÂ∏ÉÂºèÂ≠òÂÇ®Á≥ªÁªüÔºåÂ¶ÇÔºö

- **ÂØπË±°Â≠òÂÇ®ÔºàS3 ÂÖºÂÆπÔºâ**ÔºöMinIO„ÄÅCeph
- **ÂàÜÂ∏ÉÂºèÊñá‰ª∂Â≠òÂÇ®**ÔºöHDFS on Kubernetes
- **ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì**ÔºöPostgreSQL„ÄÅMySQL
- **NoSQL Êï∞ÊçÆÂ∫ì**ÔºöCassandra„ÄÅMongoDB„ÄÅElasticsearch

**Á§∫‰æãÊû∂ÊûÑÔºöHDFS on Kubernetes**

Âú® K8s ÈõÜÁæ§‰∏≠ËøêË°å **HDFS**ÔºåÂèØ‰ª•Âä®ÊÄÅÊâ©Â±ï DataNode ‰ª•Â≠òÂÇ® PB Á∫ßÊï∞ÊçÆÔºåÂπ∂ÁªìÂêà PVCÔºàPersistent Volume ClaimÔºâÁÆ°ÁêÜÂ≠òÂÇ®ËµÑÊ∫ê„ÄÇ

## ÊâπÂ§ÑÁêÜÊï∞ÊçÆËÆ°ÁÆó

<p align="center">
    <img src="./image/blog1-8.png" alt="image" width="40%">
</p>

**Apache Spark on Kubernetes**

- **Spark Driver Âú® K8s Master ‰∏äËøêË°å**ÔºåË¥üË¥£‰ªªÂä°Ë∞ÉÂ∫¶„ÄÇ
- **Spark Executors ‰Ωú‰∏∫ Pods ËøêË°å**ÔºåÂπ∂Âä®ÊÄÅ‰º∏Áº©ÔºåÊîØÊåÅÂ§ßËßÑÊ®°ËÆ°ÁÆó„ÄÇ
- **Êï∞ÊçÆÂ≠òÂÇ®Âú® HDFS / S3**ÔºåËÆ°ÁÆó‰ªªÂä°Áõ¥Êé•‰ªéÂ≠òÂÇ®‰∏≠ËØªÂèñÊï∞ÊçÆ„ÄÇ

ËøôÁßçÊñπÊ≥ïÊó†ÈúÄHadoop YarnÔºåSpark Executor Áõ¥Êé•ËøêË°åÂú® K8s Pods ‰∏≠„ÄÇÂπ∂‰∏îÊîØÊåÅÂºπÊÄß‰º∏Áº©ÔºåËÆ°ÁÆó‰ªªÂä°ÂÆåÊàêÂêéÔºåËá™Âä®ÈáäÊîæËµÑÊ∫êÔºåËäÇÁúÅËÆ°ÁÆóÊàêÊú¨„ÄÇ

## ÂÆûÊó∂ÊµÅÊï∞ÊçÆÂ§ÑÁêÜ

<p align="center">
    <img src="./image/blog1-9.png" alt="image" width="40%">
</p>


**Flink on Kubernetes**

- Kafka Áîü‰∫ßÊï∞ÊçÆÊµÅ
- **Flink Âú® K8s ÈõÜÁæ§‰∏≠ËøêË°å**ÔºåTaskManagers ‰Ωú‰∏∫ Pods ÊâßË°åÊµÅËÆ°ÁÆó
- ËÆ°ÁÆóÁªìÊûúÂ≠òÂÖ• NoSQL ÊàñÊï∞ÊçÆÊπñÔºàS3„ÄÅHDFSÔºâ

ËøôÁßçÊñπÊ≥ï‰øùËØÅ‰∫ÜÈ´òÂèØÁî®ÔºåÂ¶ÇÊûúFlink PodÂ¥©Ê∫ÉÔºåK8SÂ∞ÜËá™Âä®ÈáçÂêØÔºåÂêåÊó∂ÂèØ‰ª•Ê†πÊçÆÊï∞ÊçÆÊµÅÈáèÊâ©Â±ïFlinkÁöÑËÆ°ÁÆóËäÇÁÇπ„ÄÇ

---